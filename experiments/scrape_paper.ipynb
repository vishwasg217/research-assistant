{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "paper_url = \"https://ar5iv.labs.arxiv.org/html/2103.10360\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<h1>GLM: General Language Model Pretraining \n",
      "<br/>with Autoregressive Blank Infilling</h1>\n",
      "Zhengxiao Du<sup>∗1,2</sup>  Yujie Qian<sup>∗3</sup>  Xiao Liu<sup>1,2</sup>  Ming Ding<sup>1,2</sup>  Jiezhong Qiu<sup>1,2</sup>\n",
      "<br/>Zhilin Yang<sup>2</sup><sup>2</sup>2Corresponding authors.<math></math>  Jie Tang<sup>2</sup><sup>2</sup>2Corresponding authors.<math></math>\n",
      "<br/><sup>1</sup>Tsinghua University  <sup>2</sup>Beijing Academy of Artificial Intelligence (BAAI)\n",
      "<br/><sup>3</sup>MIT CSAIL  <sup>4</sup>Shanghai Qi Zhi Institute\n",
      "<br/>zx-du20@mails.tsinghua.edu.cn yujieq@csail.mit.edu\n",
      "<br/>{zhiliny,jietang}@tsinghua.edu.cn\n",
      "<h6>Abstract</h6>\n",
      "<p>There have been various types of pretraining architectures including autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and encoder-decoder models (e.g., T5). However, none of the pretraining frameworks performs the best for all tasks of three main categories including natural language understanding (NLU), unconditional generation, and conditional generation. We propose a General Language Model (GLM) based on autoregressive blank infilling to address this challenge.\n",
      "GLM improves blank filling pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains over BERT and T5 on NLU tasks.\n",
      "Meanwhile, GLM can be pretrained for different types of tasks by varying the number and lengths of blanks.\n",
      "On a wide range of tasks across NLU, conditional and unconditional generation, GLM outperforms BERT, T5, and GPT given the same model sizes and data, and achieves the best performance from a single pretrained model with 1.25<math></math> parameters of BERT<math></math>, demonstrating its generalizability to different downstream tasks.<sup>1</sup><sup>1</sup>1The code and pre-trained models are available at <a href=\"https://github.com/THUDM/GLM\">https://github.com/THUDM/GLM</a></p>\n",
      "<sup>1</sup><sup>1</sup>footnotetext: The first two authors contributed equally.\n",
      "<section>\n",
      "<h2>\n",
      "1 Introduction</h2>\n",
      "<p>Language models pretrained on unlabeled texts have substantially advanced the state of the art in various NLP tasks, ranging from natural language understanding (NLU) to text generation <cite>Radford et al. (<a href=\"#bib.bib28\">2018a</a>); Devlin et al. (<a href=\"#bib.bib9\">2019</a>); Yang et al. (<a href=\"#bib.bib46\">2019</a>); Radford et al. (<a href=\"#bib.bib29\">2018b</a>); Raffel et al. (<a href=\"#bib.bib30\">2020</a>); Lewis et al. (<a href=\"#bib.bib18\">2019</a>); Brown et al. (<a href=\"#bib.bib4\">2020</a>)</cite>. Downstream task performance as well as the scale of the parameters have also constantly increased in the past few years.</p>\n",
      "<figure><img src=\"/html/2103.10360/assets/x1.png\"/>\n",
      "<figcaption>Figure 1: Illustration of GLM. We blank out text spans (green part) and generate them autoregressively. (Some attention edges are omitted; cf. Figure <a href=\"#S1.F2\">2</a>.)</figcaption>\n",
      "</figure>\n",
      "<p>In general, existing pretraining frameworks can be categorized into three families: autoregressive, autoencoding, and encoder-decoder models. Autoregressive models, such as GPT <cite>Radford et al. (<a href=\"#bib.bib28\">2018a</a>)</cite>, learn left-to-right language models. While they succeed in long-text generation and show few-shot learning ability when scaled to billions of parameters <cite>Radford et al. (<a href=\"#bib.bib29\">2018b</a>); Brown et al. (<a href=\"#bib.bib4\">2020</a>)</cite>, the inherent disadvantage is the unidirectional attention mechanism, which cannot fully capture the dependencies between the context words in NLU tasks. Autoencoding models, such as BERT <cite>Devlin et al. (<a href=\"#bib.bib9\">2019</a>)</cite>, learn bidirectional context encoders via denoising objectives, e.g. Masked Language Model (MLM). The encoders\n",
      "produce contextualized representations that suit natural language understanding tasks, but could not be directly applied for text generation. Encoder-decoder models adopt bidirectional attention for the encoder, unidirectional attention for the decoder, and cross attention between them <cite>Song et al. (<a href=\"#bib.bib41\">2019</a>); Bi et al. (<a href=\"#bib.bib3\">2020</a>); Lewis et al. (<a href=\"#bib.bib18\">2019</a>)</cite>. They are typically deployed in conditional generation tasks, such as text summarization and response generation.\n",
      "<sup>2</sup><sup>2</sup>2Unconditional generation refers to generating text as a language model without finetuning, while conditional generation refers to sequence-to-sequence tasks..\n",
      "T5 <cite>Raffel et al. (<a href=\"#bib.bib30\">2020</a>)</cite> unifies NLU and conditional generation via encoder-decoder models but requires more parameters to match the performance of BRET-based models such as RoBERTa <cite>Liu et al. (<a href=\"#bib.bib21\">2019</a>)</cite> and DeBERTa <cite>He et al. (<a href=\"#bib.bib14\">2021</a>)</cite>.</p>\n",
      "<p>None of these pretraining frameworks is flexible enough to perform competitively across all NLP tasks. Previous works have tried to unify different frameworks by combining their objectives via multi-task learning <cite>Dong et al. (<a href=\"#bib.bib11\">2019</a>); Bao et al. (<a href=\"#bib.bib2\">2020</a>)</cite>. However, since the autoencoding and autoregressive objectives differ by nature, a simple unification cannot fully inherit the advantages of both frameworks.</p>\n",
      "<p>In this paper, we propose a pretraining framework named GLM (General Language Model), based on autoregressive blank infilling. We randomly blank out continuous spans of tokens from the input text, following the idea of autoencoding, and train the model to sequentially reconstruct the spans, following the idea of autoregressive pretraining (see <a href=\"#S1.F1\">Figure 1</a>). While blanking filling has been used in T5 <cite>Raffel et al. (<a href=\"#bib.bib30\">2020</a>)</cite> for text-to-text pretraining, we propose two improvements, namely span shuffling and 2D positional encoding. Empirically, we show that with the same amount of parameters and computational cost, GLM significantly outperforms BERT on the SuperGLUE benchmark by a large margin of 4.6% – 5.0% and outperforms RoBERTa and BART when pretrained on a corpus of similar size (158GB). GLM also significantly outperforms T5 on NLU and generation tasks with fewer parameters and data.</p>\n",
      "<p>Inspired by Pattern-Exploiting Training (PET) <cite>Schick and Schütze (<a href=\"#bib.bib35\">2020a</a>)</cite>, we reformulate NLU tasks as manually-crafted cloze questions that mimic human language. Different from the BERT-based models used by PET, GLM can naturally handle multi-token answers to the cloze question via autoregressive blank filling.</p>\n",
      "<p>Furthermore, we show that by varying the number and lengths of missing spans, the autoregressive blank filling objective can pretrain language models for conditional and unconditional generation. Through multi-task learning of different pretraining objectives, a single GLM can excel in both NLU and (conditional and unconditional) text generation.\n",
      "Empirically, compared with standalone baselines, GLM with multi-task pretraining achieves improvements in NLU, conditional text generation, and language modeling tasks altogether by sharing the parameters.</p>\n",
      "<figure><img src=\"/html/2103.10360/assets/x2.png\"/>\n",
      "<figcaption>Figure 2: GLM pretraining. (a) The original text is <math></math>. Two spans <math></math> and <math></math> are sampled. (b) Replace the sampled spans with [M] in Part A, and shuffle the spans in Part B. (c) GLM autoregressively generates Part B. Each span is prepended with [S] as input and appended with [E] as output. 2D positional encoding represents inter- and intra-span positions. (d) Self-attention mask. Grey areas are masked out. Part A tokens can attend to themselves (blue frame) but not B. Part B tokens can attend to A and their antecedents in B (yellow and green frames correspond to the two spans). <math></math>, <math></math>, and <math></math>.</figcaption>\n",
      "</figure>\n",
      "</section>\n",
      "<section>\n",
      "<h2>\n",
      "2 GLM Pretraining Framework</h2>\n",
      "<p>We propose a general pretraining framework GLM based on a novel autoregressive blank infilling objective. GLM formulates NLU tasks as cloze questions that contain task descriptions, which can be answered by autoregressive generation.</p>\n",
      "<section>\n",
      "<h3>\n",
      "2.1 Pretraining Objective</h3>\n",
      "<section>\n",
      "<h4>\n",
      "2.1.1 Autoregressive Blank Infilling</h4>\n",
      "<p>GLM is trained by optimizing an autoregressive blank infilling objective. Given an input text <math></math>, multiple text spans <math></math> are sampled, where each span <math></math> corresponds to a series of consecutive tokens <math></math> in <math></math>. Each span is replaced with a single <math></math> token, forming a corrupted text <math></math>. The model predicts the missing tokens in the spans from the corrupted text in an autoregressive manner, which means when predicting the missing tokens in a span, the model has access to the corrupted text <em>and</em> the previously predicted spans. To fully capture the interdependencies between different spans, we randomly permute the order of the spans, similar to the permutation language model <cite>Yang et al. (<a href=\"#bib.bib46\">2019</a>)</cite>.\n",
      "Formally, let <math></math> be the set of all possible permutations of the length-<math></math> index sequence <math></math>, and <math></math> be <math></math>, we define the pretraining objective as</p>\n",
      "<table>\n",
      "<tbody><tr>\n",
      "<td></td>\n",
      "<td><math></math></td>\n",
      "<td></td>\n",
      "<td>(1)</td>\n",
      "</tr></tbody>\n",
      "</table>\n",
      "<p>We always generate the tokens in each blank following a left-to-right order, i.e. the probability of generating the span <math></math> is factorized as:</p>\n",
      "<table>\n",
      "<tbody><tr>\n",
      "<td></td>\n",
      "<td><math></math></td>\n",
      "<td></td>\n",
      "<td>(2)</td>\n",
      "</tr></tbody>\n",
      "</table>\n",
      "<p>We implement the autoregressive blank infilling objective with the following techniques. The input <math></math> is divided into two parts: Part A is the corrupted text <math></math>, and Part B consists of the masked spans. Part A tokens can attend to each other, but cannot attend to any tokens in B. Part B tokens can attend to Part A and antecedents in B, but cannot attend to any subsequent tokens in B. To enable autoregressive generation, each span is padded with special tokens <math></math> and <math></math>, for input and output respectively. In this way, our model automatically learns a bidirectional encoder (for Part A) and a unidirectional decoder (for Part B) in a unified model. The implementation of GLM is illustrated in <a href=\"#S1.F2\">Figure 2</a>.</p>\n",
      "<p>We randomly sample spans of length drawn from a Poisson distribution with <math></math>. We repeatedly sample new spans until at least 15% of the original tokens are masked. Empirically, we have found that the 15% ratio is critical for good performance on downstream NLU tasks.</p>\n",
      "</section>\n",
      "<section>\n",
      "<h4>\n",
      "2.1.2 Multi-Task Pretraining</h4>\n",
      "<p>In the previous section, GLM masks short spans and is suited for NLU tasks. However, we are interested in pretraining a single model that can handle both NLU and text generation. We then study a multi-task pretraining setup, in which a second objective of generating longer text is jointly optimized with the blank infilling objective. We consider the following two objectives:</p>\n",
      "<ul>\n",
      "<li>\n",
      "<p>Document-level. We sample a single span whose length is sampled from a uniform distribution over 50%–100% of the original length. The objective aims for long text generation.</p>\n",
      "</li>\n",
      "<li>\n",
      "<p>Sentence-level. We restrict that the masked spans must be full sentences. Multiple spans (sentences) are sampled to cover 15% of the original tokens. This objective aims for seq2seq tasks whose predictions are often complete sentences or paragraphs.</p>\n",
      "</li>\n",
      "</ul>\n",
      "<p>Both new objectives are defined in the same way as the original objective, i.e. Eq. <a href=\"#S2.E1\">1</a>. The only difference is the number of spans and the span lengths.</p>\n",
      "</section>\n",
      "</section>\n",
      "<section>\n",
      "<h3>\n",
      "2.2 Model Architecture</h3>\n",
      "<p>GLM uses a single Transformer with several modifications to the architecture: (1) we rearrange the order of layer normalization and the residual connection, which has been shown critical for large-scale language models to avoid numerical errors <cite>Shoeybi et al. (<a href=\"#bib.bib39\">2019</a>)</cite>; (2) we use a single linear layer for the output token prediction; (3) we replace ReLU activation functions with GeLUs <cite>Hendrycks and Gimpel (<a href=\"#bib.bib15\">2016</a>)</cite>.</p>\n",
      "<section>\n",
      "<h4>\n",
      "2.2.1 2D Positional Encoding</h4>\n",
      "<p>One of the challenges of the autoregressive blank infilling task is how to encode the positional information. Transformers rely on positional encodings to inject the absolute and relative positions of the tokens.\n",
      "We propose 2D positional encodings to address the challenge. Specifically, each token is encoded with two positional ids. The first positional id represents the position in the corrupted text <math></math>. For the masked spans, it is the position of the corresponding <math></math> token. The second positional id represents the intra-span position. For tokens in Part A, their second positional ids are <math></math>. For tokens in Part B, they range from 1 to the length of the span. The two positional ids are projected into two vectors via learnable embedding tables, which are both added to the input token embeddings.</p>\n",
      "<p>Our encoding method ensures that the model is not aware of the length of the masked span when reconstructing them. It is an important difference as compared to other models. For example, XLNet <cite>Yang et al. (<a href=\"#bib.bib46\">2019</a>)</cite> encodes the original position so that it can perceive the number of missing tokens, and SpanBERT <cite>Joshi et al. (<a href=\"#bib.bib16\">2020</a>)</cite> replaces the span with multiple [MASK] tokens and keeps the length unchanged. Our design fits downstream tasks as usually the length of the generated text is unknown beforehand.</p>\n",
      "<figure><img src=\"/html/2103.10360/assets/x3.png\"/>\n",
      "<figcaption>Figure 3: Formulation of the sentiment classification task as blank infilling with GLM.</figcaption>\n",
      "</figure>\n",
      "</section>\n",
      "</section>\n",
      "<section>\n",
      "<h3>\n",
      "2.3 Finetuning GLM</h3>\n",
      "<p>Typically, for downstream NLU tasks, a linear classifier takes the representations of sequences or tokens produced by pretrained models as input and predicts the correct labels.\n",
      "The practices are different from the generative pretraining task, leading to inconsistency between pretraining and finetuning.</p>\n",
      "<p>Instead, we reformulate NLU classification tasks as generation tasks of blank infilling, following PET <cite>Schick and Schütze (<a href=\"#bib.bib35\">2020a</a>)</cite>. Specifically, given a labeled example <math></math>, we convert the input text <math></math> to a cloze question <math></math> via a pattern containing a single mask token. The pattern is written in natural language to represent the semantics of the task. For example, a sentiment classification task can be formulated as “{SENTENCE}. It’s really <math></math>”. The candidate labels <math></math> are also mapped to answers to the cloze, called verbalizer <math></math>. In sentiment classification, the labels “positive” and “negative” are mapped to the words “good” and “bad”. The conditional probability of predicting <math></math> given <math></math> is</p>\n",
      "<table>\n",
      "<tbody><tr>\n",
      "<td></td>\n",
      "<td><math></math></td>\n",
      "<td></td>\n",
      "<td>(3)</td>\n",
      "</tr></tbody>\n",
      "</table>\n",
      "<p>where <math></math> is the label set. Therefore the probability of the sentence being positive or negative is proportional to predicting “good” or “bad” in the blank. Then we finetune GLM with a cross-entropy loss (see <a href=\"#S2.F3\">Figure 3</a>).</p>\n",
      "<p>For text generation tasks, the given context constitutes the Part A of the input, with a mask token appended at the end. The model generates the text of Part B autoregressively. We can directly apply the pretrained GLM for unconditional generation, or finetune it on downstream conditional generation tasks.</p>\n",
      "</section>\n",
      "<section>\n",
      "<h3>\n",
      "2.4 Discussion and Analysis</h3>\n",
      "<p>In this section, we discuss the differences between GLM and other pretraining models. We are mainly concerned with how they can be adapted to downstream blank infilling tasks.</p>\n",
      "<p>Comparison with BERT <cite>Devlin et al. (<a href=\"#bib.bib9\">2019</a>)</cite>.\n",
      "As pointed out by <cite>Yang et al. (<a href=\"#bib.bib46\">2019</a>)</cite>, BERT fails to capture the interdependencies of masked tokens due to the independence assumption of MLM. Another disadvantage of BERT is that it cannot fill in the blanks of multiple tokens properly. To infer the probability of an answer of length <math></math>, BERT needs to perform <math></math> consecutive predictions. If the length <math></math> is unknown, we may need to enumerate all possible lengths, since BERT needs to change the number of <math></math> tokens according to the length.</p>\n",
      "<p>Comparison with XLNet <cite>Yang et al. (<a href=\"#bib.bib46\">2019</a>)</cite>.\n",
      "Both GLM and XLNet are pretrained with autoregressive objectives, but there are two differences between them. First, XLNet uses the original position encodings before corruption. During inference, we need to either know or enumerate the length of the answer, the same problem as BERT. Second, XLNet uses a two-stream self-attention mechanism, instead of the right-shift, to avoid the information leak within Transformer. It doubles the time cost of pretraining.</p>\n",
      "<p>Comparison with T5 <cite>Raffel et al. (<a href=\"#bib.bib30\">2020</a>)</cite>.\n",
      "T5 proposes a similar blank infilling objective to pretrain an encoder-decoder Transformer.\n",
      "T5 uses independent positional encodings for the encoder and decoder, and relies on multiple sentinel tokens to differentiate the masked spans. In downstream tasks, only one of the sentinel tokens is used, leading to a waste of model capacity and inconsistency between pretraining and finetuning. Moreover, T5 always predicts spans in a fixed left-to-right order. As a result, GLM can significantly outperform T5 on NLU and seq2seq tasks with fewer parameters and data, as stated in <a href=\"#S3.SS2\">Sections 3.2</a> and <a href=\"#S3.SS3\">3.3</a>.</p>\n",
      "<p>Comparison with UniLM <cite>Dong et al. (<a href=\"#bib.bib11\">2019</a>)</cite>.\n",
      "UniLM combines different pretraining objectives under the autoencoding framework by changing the attention mask among bidirectional, unidirectional, and cross attention. However, UniLM always replaces masked spans with [MASK] tokens, which limits its ability to model the dependencies between the masked spans and their context. GLM feeds in the previous token and autoregressively generates the next token. Finetuning UniLM on downstream generation tasks also relies on masked language modeling, which is less efficient. UniLMv2 <cite>Bao et al. (<a href=\"#bib.bib2\">2020</a>)</cite> adopts partially autoregressive modeling for generation tasks, along with the autoencoding objective for NLU tasks. Instead, GLM unifies NLU and generation tasks with autoregressive pretraining.</p>\n",
      "<figure>\n",
      "<figcaption>Table 1: Results on the SuperGLUE dev set.</figcaption>\n",
      "<table>\n",
      "<tr>\n",
      "<td></td>\n",
      "<td>Model</td>\n",
      "<td>\n",
      " \n",
      "ReCoRD\n",
      "F1/Acc.\n",
      "</td>\n",
      "<td>\n",
      " \n",
      "COPA\n",
      "Acc.\n",
      "</td>\n",
      "<td>\n",
      " \n",
      "WSC\n",
      "Acc.\n",
      "</td>\n",
      "<td>\n",
      " \n",
      "RTE\n",
      "Acc.\n",
      "</td>\n",
      "<td>\n",
      " \n",
      "BoolQ\n",
      "Acc.\n",
      "</td>\n",
      "<td>\n",
      " \n",
      "WiC\n",
      "Acc.\n",
      "</td>\n",
      "<td>\n",
      " \n",
      "CB\n",
      "F1/Acc.\n",
      "</td>\n",
      "<td>\n",
      " \n",
      "MultiRC\n",
      "F1a/EM\n",
      "</td>\n",
      "<td>Avg</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Pretrained on BookCorpus and Wikipedia</td>\n",
      "<td></td>\n",
      "<td></td>\n",
      "<td></td>\n",
      "<td></td>\n",
      "<td></td>\n",
      "<td></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td></td>\n",
      "<td>BERT<math></math>\n",
      "</td>\n",
      "<td>65.4 / 64.9</td>\n",
      "<td>66.0</td>\n",
      "<td>65.4</td>\n",
      "<td>70.0</td>\n",
      "<td>74.9</td>\n",
      "<td>68.8</td>\n",
      "<td>70.9 / 76.8</td>\n",
      "<td>68.4 / 21.5</td>\n",
      "<td>66.1</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td></td>\n",
      "<td>GLM<math></math>\n",
      "</td>\n",
      "<td>73.5 / 72.8</td>\n",
      "<td>71.0</td>\n",
      "<td>72.1</td>\n",
      "<td>71.2</td>\n",
      "<td>77.0</td>\n",
      "<td>64.7</td>\n",
      "<td>89.5 / 85.7</td>\n",
      "<td>72.1 / 26.1</td>\n",
      "<td>70.7</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td></td>\n",
      "<td>BERT<math></math>\n",
      "</td>\n",
      "<td>76.3 / 75.6</td>\n",
      "<td>69.0</td>\n",
      "<td>64.4</td>\n",
      "<td>73.6</td>\n",
      "<td>80.1</td>\n",
      "<td>71.0</td>\n",
      "<td>94.8 / 92.9</td>\n",
      "<td>71.9 / 24.1</td>\n",
      "<td>72.0</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td></td>\n",
      "<td>UniLM<math></math>\n",
      "</td>\n",
      "<td>80.0 / 79.1</td>\n",
      "<td>72.0</td>\n",
      "<td>65.4</td>\n",
      "<td>76.5</td>\n",
      "<td>80.5</td>\n",
      "<td>69.7</td>\n",
      "<td>91.0 / 91.1</td>\n",
      "<td>77.2 / 38.2</td>\n",
      "<td>74.1</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td></td>\n",
      "<td>GLM<math></math>\n",
      "</td>\n",
      "<td>81.7 / 81.1</td>\n",
      "<td>76.0</td>\n",
      "<td>81.7</td>\n",
      "<td>74.0</td>\n",
      "<td>82.1</td>\n",
      "<td>68.5</td>\n",
      "<td>96.1 / 94.6</td>\n",
      "<td>77.1 / 36.3</td>\n",
      "<td>77.0</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td></td>\n",
      "<td>GLM<math></math>\n",
      "</td>\n",
      "<td>80.2 / 79.6</td>\n",
      "<td>77.0</td>\n",
      "<td>78.8</td>\n",
      "<td>76.2</td>\n",
      "<td>79.8</td>\n",
      "<td>63.6</td>\n",
      "<td>97.3 / 96.4</td>\n",
      "<td>74.6 / 32.1</td>\n",
      "<td>75.7</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td></td>\n",
      "<td>GLM<math></math>\n",
      "</td>\n",
      "<td>80.7 / 80.2</td>\n",
      "<td>77.0</td>\n",
      "<td>79.8</td>\n",
      "<td>79.1</td>\n",
      "<td>80.8</td>\n",
      "<td>70.4</td>\n",
      "<td>94.6 / 93.7</td>\n",
      "<td>76.9 / 36.1</td>\n",
      "<td>76.8</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td></td>\n",
      "<td>GLM<math></math>\n",
      "</td>\n",
      "<td>81.5 / 80.9</td>\n",
      "<td>80.0</td>\n",
      "<td>81.7</td>\n",
      "<td>79.4</td>\n",
      "<td>81.9</td>\n",
      "<td>69.0</td>\n",
      "<td>93.2 / 96.4\n",
      "</td>\n",
      "<td>76.2 / 35.5</td>\n",
      "<td>78.0</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td></td>\n",
      "<td>GLM<math></math>\n",
      "</td>\n",
      "<td>82.3 / 81.7</td>\n",
      "<td>85.0</td>\n",
      "<td>81.7</td>\n",
      "<td>79.1</td>\n",
      "<td>81.3</td>\n",
      "<td>69.4</td>\n",
      "<td>95.0 / 96.4\n",
      "</td>\n",
      "<td>\n",
      "77.2 / 35.0</td>\n",
      "<td>78.8</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Pretrained on larger corpora</td>\n",
      "<td></td>\n",
      "<td></td>\n",
      "<td></td>\n",
      "<td></td>\n",
      "<td></td>\n",
      "<td></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td></td>\n",
      "<td>T5<math></math>\n",
      "</td>\n",
      "<td>76.2 / 75.4</td>\n",
      "<td>73.0</td>\n",
      "<td>79.8</td>\n",
      "<td>78.3</td>\n",
      "<td>80.8</td>\n",
      "<td>67.9</td>\n",
      "<td>94.8 / 92.9</td>\n",
      "<td>76.4 / 40.0</td>\n",
      "<td>76.0</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td></td>\n",
      "<td>T5<math></math>\n",
      "</td>\n",
      "<td>85.7 / 85.0</td>\n",
      "<td>78.0</td>\n",
      "<td>84.6</td>\n",
      "<td>84.8</td>\n",
      "<td>84.3</td>\n",
      "<td>71.6</td>\n",
      "<td>96.4 / 98.2</td>\n",
      "<td>80.9 / 46.6</td>\n",
      "<td>81.2</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td></td>\n",
      "<td>BART<math></math>\n",
      "</td>\n",
      "<td>88.3 / 87.8</td>\n",
      "<td>60.0</td>\n",
      "<td>65.4</td>\n",
      "<td>84.5</td>\n",
      "<td>84.3</td>\n",
      "<td>69.0</td>\n",
      "<td>90.5 / 92.9</td>\n",
      "<td>81.8 / 48.0</td>\n",
      "<td>76.0</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td></td>\n",
      "<td>RoBERTa<math></math>\n",
      "</td>\n",
      "<td>89.0 / 88.4</td>\n",
      "<td>90.0</td>\n",
      "<td>63.5</td>\n",
      "<td>87.0</td>\n",
      "<td>86.1</td>\n",
      "<td>72.6</td>\n",
      "<td>96.1 / 94.6</td>\n",
      "<td>84.4 / 52.9</td>\n",
      "<td>81.5</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td></td>\n",
      "<td>GLM<math></math>\n",
      "</td>\n",
      "<td>89.6 / 89.0</td>\n",
      "<td>82.0</td>\n",
      "<td>83.7</td>\n",
      "<td>87.7</td>\n",
      "<td>84.7</td>\n",
      "<td>71.2</td>\n",
      "<td>98.7 / 98.2</td>\n",
      "<td>82.4 / 50.1</td>\n",
      "<td>82.9</td>\n",
      "</tr>\n",
      "</table>\n",
      "</figure>\n",
      "</section>\n",
      "</section>\n",
      "<section>\n",
      "<h2>\n",
      "3 Experiments</h2>\n",
      "<p>We now describe our pretraining setup and the evaluation of downstream tasks.</p>\n",
      "<section>\n",
      "<h3>\n",
      "3.1 Pretraining Setup</h3>\n",
      "<p>For a fair comparison with BERT <cite>Devlin et al. (<a href=\"#bib.bib9\">2019</a>)</cite>, we use BooksCorpus <cite>Zhu et al. (<a href=\"#bib.bib50\">2015</a>)</cite> and English Wikipedia as our pretraining data. We use the uncased wordpiece tokenizer of BERT with 30k vocabulary.\n",
      "We train GLM<math></math> and GLM<math></math> with the same architectures as BERT<math></math> and BERT<math></math>, containing 110M and 340M parameters respectively.</p>\n",
      "<p>For multi-task pretraining, we train two Large-sized models with a mixture of the blank infilling objective and the document-level or sentence-level objective, denoted as GLM<math></math> and GLM<math></math>. Additionally, we train two larger GLM models of 410M (30 layers, hidden size 1024, and 16 attention heads) and 515M (30 layers, hidden size 1152, and 18 attention heads) parameters with document-level multi-task pretraining, denoted as GLM<math></math> and GLM<math></math>.</p>\n",
      "<p>To compare with SOTA models, we also train a Large-sized model with the same data, tokenization, and hyperparameters as RoBERTa <cite>Liu et al. (<a href=\"#bib.bib21\">2019</a>)</cite>, denoted as GLM<math></math>.\n",
      "Due to resource limitations, we only pretrain the model for 250,000 steps, which are half of RoBERTa and BART’s training steps and close to T5 in the number of trained tokens.\n",
      "More experiment details can be found in <a href=\"#A1\">Appendix A</a>.</p>\n",
      "<figure>\n",
      "<figcaption>Table 2: Results of abstractive summarization on the CNN/DailyMail and XSum test sets.</figcaption>\n",
      "<table>\n",
      "<tbody>\n",
      "<tr>\n",
      "<th>\n",
      " \n",
      "Model\n",
      "</th>\n",
      "<td>CNN/DailyMail</td>\n",
      "<td>XSum</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>RG-1</td>\n",
      "<td>RG-2</td>\n",
      "<td>RG-L</td>\n",
      "<td>RG-1</td>\n",
      "<td>RG-2</td>\n",
      "<td>RG-L</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>BERTSumAbs <cite>Liu and Lapata (<a href=\"#bib.bib20\">2019</a>)</cite>\n",
      "</td>\n",
      "<td>41.7</td>\n",
      "<td>19.4</td>\n",
      "<td>38.8</td>\n",
      "<td>38.8</td>\n",
      "<td>16.3</td>\n",
      "<td>31.2</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>UniLMv2<math></math> <cite>Bao et al. (<a href=\"#bib.bib2\">2020</a>)</cite>\n",
      "</td>\n",
      "<td>43.2</td>\n",
      "<td>20.4</td>\n",
      "<td>40.1</td>\n",
      "<td>44.0</td>\n",
      "<td>21.1</td>\n",
      "<td>36.1</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>T5<math></math> <cite>Raffel et al. (<a href=\"#bib.bib30\">2020</a>)</cite>\n",
      "</td>\n",
      "<td>42.5</td>\n",
      "<td>20.7</td>\n",
      "<td>39.8</td>\n",
      "<td>40.9</td>\n",
      "<td>17.3</td>\n",
      "<td>33.0</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>BART<math></math> <cite>Lewis et al. (<a href=\"#bib.bib18\">2019</a>)</cite>\n",
      "</td>\n",
      "<td>44.2</td>\n",
      "<td>21.3</td>\n",
      "<td>40.9</td>\n",
      "<td>45.1</td>\n",
      "<td>22.3</td>\n",
      "<td>37.3</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>GLM<math></math>\n",
      "</td>\n",
      "<td>43.8</td>\n",
      "<td>21.0</td>\n",
      "<td>40.5</td>\n",
      "<td>45.5</td>\n",
      "<td>23.5</td>\n",
      "<td>37.3</td>\n",
      "</tr>\n",
      "</tbody>\n",
      "</table>\n",
      "</figure>\n",
      "</section>\n",
      "<section>\n",
      "<h3>\n",
      "3.2 SuperGLUE</h3>\n",
      "<p>To evaluate our pretrained GLM models, we conduct experiments on the SuperGLUE benchmark <cite>Wang et al. (<a href=\"#bib.bib43\">2019</a>)</cite> and report the standard metrics. SuperGLUE consists of 8 challenging NLU tasks.\n",
      "We reformulate the classification tasks as blank infilling with human-crafted cloze questions, following PET <cite>Schick and Schütze (<a href=\"#bib.bib36\">2020b</a>)</cite>. Then we finetune the pretrained GLM models on each task as described in <a href=\"#S2.SS3\">Section 2.3</a>. The cloze questions and other details can be found in <a href=\"#A2.SS1\">Section B.1</a>.</p>\n",
      "<p>For a fair comparison with GLM<math></math> and GLM<math></math>, we choose BERT<math></math> and BERT<math></math> as our baselines, which are pretrained on the same corpus and for a similar amount of time. We report the performance of standard finetuning (i.e. classification on the [CLS] token representation). The performance of BERT with cloze questions is reported in <a href=\"#S3.SS4\">Section 3.4</a>. To compare with GLM<math></math>, we choose T5, BART<math></math>, and RoBERTa<math></math> as our baselines. T5 has no direct match in the number of parameters for BERT<math></math>, so we present the results of both T5<math></math> (220M parameters) and T5<math></math> (770M parameters). All the other baselines are of similar size to BERT<math></math>.</p>\n",
      "<p><a href=\"#S2.T1\">Table 1</a> shows the results. With the same amount of training data, GLM consistently outperforms BERT on most tasks with either base or large architecture. The only exception is WiC (word sense disambiguation). On average, GLM<math></math> scores 4.6% higher than BERT<math></math>, and GLM<math></math> scores 5.0% higher than BERT<math></math>. It clearly demonstrates the advantage of our method in NLU tasks. In the setting of RoBERTa<math></math>, GLM<math></math> can still achieve improvements over the baselines, but with a smaller margin. Specifically, GLM<math></math> outperforms T5<math></math> but is only half its size. We also find that BART does not perform well on the challenging SuperGLUE benchmark. We conjecture this can be attributed to the low parameter efficiency of the encoder-decoder architecture and the denoising sequence-to-sequence objective.</p>\n",
      "</section>\n",
      "<section>\n",
      "<h3>\n",
      "3.3 Multi-Task Pretraining</h3>\n",
      "<p>Then we evaluate the GLM’s performance in a multi-task setting (<a href=\"#S2.SS1\">Section 2.1</a>). Within one training batch, we sample short spans and longer spans (document-level or sentence-level) with equal chances. We evaluate the multi-task model for NLU, seq2seq, blank infilling, and zero-shot language modeling.</p>\n",
      "<p>SuperGLUE.\n",
      "For NLU tasks, we evaluate models on the SuperGLUE benchmark. The results are also shown in <a href=\"#S2.T1\">Table 1</a>. We observe that with multi-task pretraining, GLM<math></math> and GLM<math></math> perform slightly worse than GLM<math></math>, but still outperform BERT<math></math> and UniLM<math></math>. Among multi-task models, GLM<math></math> outperforms GLM<math></math> by 1.1% on average. Increasing GLM<math></math>’s parameters to 410M (1.25<math></math>BERT<math></math>) leads to better performance than GLM<math></math>. GLM with 515M parameters (1.5<math></math>BERT<math></math>) can perform even better.</p>\n",
      "<figure>\n",
      "<figcaption>Table 3: Results on Gigaword summarization.</figcaption>\n",
      "<table>\n",
      "<tr>\n",
      "<td>Model</td>\n",
      "<td>RG-1</td>\n",
      "<td>RG-2</td>\n",
      "<td>RG-L</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>MASS</td>\n",
      "<td>37.7</td>\n",
      "<td>18.5</td>\n",
      "<td>34.9</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>UniLM<math></math>\n",
      "</td>\n",
      "<td>38.5</td>\n",
      "<td>19.5</td>\n",
      "<td>35.8</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>GLM<math></math>\n",
      "</td>\n",
      "<td>38.6</td>\n",
      "<td>19.7</td>\n",
      "<td>36.0</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>GLM<math></math>\n",
      "</td>\n",
      "<td>38.5</td>\n",
      "<td>19.4</td>\n",
      "<td>35.8</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>GLM<math></math>\n",
      "</td>\n",
      "<td>38.9</td>\n",
      "<td>20.0</td>\n",
      "<td>36.3</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>GLM<math></math>\n",
      "</td>\n",
      "<td>38.9</td>\n",
      "<td>20.0</td>\n",
      "<td>36.2</td>\n",
      "</tr>\n",
      "</table>\n",
      "<figcaption>Table 4: Results on SQuAD question generation.</figcaption>\n",
      "<table>\n",
      "<tr>\n",
      "<td>Model</td>\n",
      "<td>BLEU-4</td>\n",
      "<td>MTR</td>\n",
      "<td>RG-L</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>SemQG</td>\n",
      "<td>18.4</td>\n",
      "<td>22.7</td>\n",
      "<td>46.7</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>UniLM<math></math>\n",
      "</td>\n",
      "<td>22.1</td>\n",
      "<td>25.1</td>\n",
      "<td>51.1</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>GLM<math></math>\n",
      "</td>\n",
      "<td>22.4</td>\n",
      "<td>25.2</td>\n",
      "<td>50.4</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>GLM<math></math>\n",
      "</td>\n",
      "<td>22.3</td>\n",
      "<td>25.0</td>\n",
      "<td>50.2</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>GLM<math></math>\n",
      "</td>\n",
      "<td>22.6</td>\n",
      "<td>25.4</td>\n",
      "<td>50.4</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>GLM<math></math>\n",
      "</td>\n",
      "<td>22.9</td>\n",
      "<td>25.6</td>\n",
      "<td>50.5</td>\n",
      "</tr>\n",
      "</table>\n",
      "<figcaption>Table 5: BLEU scores on Yahoo text infilling. <sup>†</sup> indicates the results from <cite>Shen et al. (<a href=\"#bib.bib38\">2020</a>)</cite>.</figcaption>\n",
      "<table>\n",
      "<tr>\n",
      "<td>Mask ratio</td>\n",
      "<td>10%</td>\n",
      "<td>20%</td>\n",
      "<td>30%</td>\n",
      "<td>40%</td>\n",
      "<td>50%</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>BERT<sup>†</sup>\n",
      "</td>\n",
      "<td>82.8</td>\n",
      "<td>66.3</td>\n",
      "<td>50.3</td>\n",
      "<td>37.4</td>\n",
      "<td>26.2</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>BLM<sup>†</sup>\n",
      "</td>\n",
      "<td>86.5</td>\n",
      "<td>73.2</td>\n",
      "<td>59.6</td>\n",
      "<td>46.8</td>\n",
      "<td>34.8</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>GLM<math></math>\n",
      "</td>\n",
      "<td>87.8</td>\n",
      "<td>76.7</td>\n",
      "<td>64.2</td>\n",
      "<td>48.9</td>\n",
      "<td>38.7</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>GLM<math></math>\n",
      "</td>\n",
      "<td>87.5</td>\n",
      "<td>76.0</td>\n",
      "<td>63.2</td>\n",
      "<td>47.9</td>\n",
      "<td>37.6</td>\n",
      "</tr>\n",
      "</table>\n",
      "</figure>\n",
      "<p>Sequence-to-Sequence.\n",
      "Considering the available baseline results, we use the Gigaword dataset <cite>Rush et al. (<a href=\"#bib.bib34\">2015</a>)</cite> for abstractive summarization and the SQuAD 1.1 dataset <cite>Rajpurkar et al. (<a href=\"#bib.bib32\">2016</a>)</cite> for question generation <cite>Du et al. (<a href=\"#bib.bib12\">2017</a>)</cite> as the benchmarks for models pretrained on BookCorpus and Wikipedia. Additionally, we use the CNN/DailyMail <cite>See et al. (<a href=\"#bib.bib37\">2017</a>)</cite> and XSum <cite>Narayan et al. (<a href=\"#bib.bib23\">2018</a>)</cite> datasets for abstractive summarization as the benchmarks for models pretrained on larger corpora.</p>\n",
      "<p>The results for models trained on BookCorpus and Wikipedia are shown in <a href=\"#S3.T5\">Tables 5</a> and <a href=\"#S3.T5\">5</a>. We observe that GLM<math></math> can achieve performance matching the other pretraining models on the two generation tasks.\n",
      "GLM<math></math> can perform better than GLM<math></math>, while\n",
      "GLM<math></math> performs slightly worse than GLM<math></math>. This indicates that the document-level objective, which teaches the model to extend the given contexts, is less helpful to conditional generation, which aims to extract useful information from the context. Increasing GLM<math></math>’s parameters to 410M leads to the best performance on both tasks.\n",
      "The results for models trained on larger corpora are shown in <a href=\"#S3.T2\">Table 2</a>. GLM<math></math> can achieve performance matching the seq2seq BART model, and outperform T5 and UniLMv2.</p>\n",
      "<p>Text Infilling.\n",
      "Text infilling is the task of predicting missing spans of text which are consistent with the surrounding context <cite>Zhu et al. (<a href=\"#bib.bib49\">2019</a>); Donahue et al. (<a href=\"#bib.bib10\">2020</a>); Shen et al. (<a href=\"#bib.bib38\">2020</a>)</cite>. GLM is trained with an autoregressive blank infilling objective, thus can straightforwardly solve this task. We evaluate GLM on the Yahoo Answers dataset <cite>Yang et al. (<a href=\"#bib.bib47\">2017</a>)</cite> and compare it with Blank Language Model (BLM) <cite>Shen et al. (<a href=\"#bib.bib38\">2020</a>)</cite>, which is a specifically designed model for text infilling. From the results in <a href=\"#S3.T5\">Table 5</a>, GLM outperforms previous methods by large margins (1.3 to 3.9 BLEU) and achieves the state-of-the-art result on this dataset. We notice that GLM<math></math> slightly underperforms GLM<math></math>, which is consistent with our observations in the seq2seq experiments.</p>\n",
      "<p>Language Modeling.\n",
      "Most language modeling datasets such as WikiText103 are constructed from Wikipedia documents, which our pretraining dataset already contains.\n",
      "Therefore, we evaluate the language modeling perplexity on a held-out test set of our pretraining dataset, which contains about 20M tokens, denoted as BookWiki. We also evaluate GLM on the LAMBADA dataset <cite>Paperno et al. (<a href=\"#bib.bib25\">2016</a>)</cite>, which tests the ability of systems to model long-range dependencies in text. The task is to predict the final word of a passage. As the baseline, we train a GPT<math></math> model <cite>Radford et al. (<a href=\"#bib.bib29\">2018b</a>); Brown et al. (<a href=\"#bib.bib4\">2020</a>)</cite> with the same data and tokenization as GLM<math></math>.</p>\n",
      "<p>The results are shown in <a href=\"#S3.F4\">Figure 4</a>. All the models are evaluated in the zero-shot setting. Since GLM learns the bidirectional attention, we also evaluate GLM under the setting in which the contexts are encoded with bidirectional attention. Without generative objective during pretraining, GLM<math></math> cannot complete the language modeling tasks, with perplexity larger than 100. With the same amount of parameters, GLM<math></math> performs worse than GPT<math></math>. This is expected since GLM<math></math> also optimizes the blank infilling objective. Increasing the model’s parameters to 410M (1.25<math></math> of GPT<math></math>) leads to a performance close to GPT<math></math>. GLM<math></math> (1.5<math></math> of GPT<math></math>) can further outperform GPT<math></math>. With the same amount of parameters, encoding the context with bidirectional attention can improve the performance of language modeling. Under this setting, GLM<math></math> outperforms GPT<math></math>. This is the advantage of GLM over unidirectional GPT. We also study the contribution of 2D positional encoding to long text generation. We find that removing the 2D positional encoding leads to lower accuracy and higher perplexity in language modeling.</p>\n",
      "<figure><img src=\"/html/2103.10360/assets/x4.png\"/>\n",
      "<figcaption>Figure 4: Zero-shot language modeling results.</figcaption>\n",
      "</figure>\n",
      "<figure>\n",
      "<figcaption>Table 6: Ablation study on the SuperGLUE dev set. (T5 <math></math> GLM – shuffle spans + sentinel tokens.)</figcaption>\n",
      "<table>\n",
      "<tr>\n",
      "<td>Model</td>\n",
      "<td>\n",
      " \n",
      "ReCoRD\n",
      "F1/Acc.\n",
      "</td>\n",
      "<td>\n",
      " \n",
      "COPA\n",
      "Acc.\n",
      "</td>\n",
      "<td>\n",
      " \n",
      "WSC\n",
      "Acc.\n",
      "</td>\n",
      "<td>\n",
      " \n",
      "RTE\n",
      "Acc.\n",
      "</td>\n",
      "<td>\n",
      " \n",
      "BoolQ\n",
      "Acc.\n",
      "</td>\n",
      "<td>\n",
      " \n",
      "WiC\n",
      "Acc.\n",
      "</td>\n",
      "<td>\n",
      " \n",
      "CB\n",
      "F1/Acc.\n",
      "</td>\n",
      "<td>\n",
      " \n",
      "MultiRC\n",
      "F1a/EM\n",
      "</td>\n",
      "<td>Avg</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>BERT<math></math>\n",
      "</td>\n",
      "<td>76.3 / 75.6</td>\n",
      "<td>69.0</td>\n",
      "<td>64.4</td>\n",
      "<td>73.6</td>\n",
      "<td>80.1</td>\n",
      "<td>71.0</td>\n",
      "<td>94.8 / 92.9</td>\n",
      "<td>71.9 / 24.1</td>\n",
      "<td>72.0</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>BERT<math></math> (reproduced)</td>\n",
      "<td>82.1 / 81.5</td>\n",
      "<td>63.0</td>\n",
      "<td>63.5</td>\n",
      "<td>72.2</td>\n",
      "<td>80.8</td>\n",
      "<td>68.7</td>\n",
      "<td>80.9 / 85.7</td>\n",
      "<td>77.0 / 35.2</td>\n",
      "<td>71.2</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>BERT<math></math> (cloze)</td>\n",
      "<td>70.0 / 69.4</td>\n",
      "<td>80.0</td>\n",
      "<td>76.0</td>\n",
      "<td>72.6</td>\n",
      "<td>78.1</td>\n",
      "<td>70.5</td>\n",
      "<td>93.5 / 91.1</td>\n",
      "<td>70.0 / 23.1</td>\n",
      "<td>73.2</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>GLM<math></math>\n",
      "</td>\n",
      "<td>81.7 / 81.1</td>\n",
      "<td>76.0</td>\n",
      "<td>81.7</td>\n",
      "<td>74.0</td>\n",
      "<td>82.1</td>\n",
      "<td>68.5</td>\n",
      "<td>96.1 / 94.6</td>\n",
      "<td>77.1 / 36.3</td>\n",
      "<td>77.0</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>     – cloze finetune</td>\n",
      "<td>81.3 / 80.6</td>\n",
      "<td>62.0</td>\n",
      "<td>63.5</td>\n",
      "<td>66.8</td>\n",
      "<td>80.5</td>\n",
      "<td>65.0</td>\n",
      "<td>89.2 / 91.1</td>\n",
      "<td>72.3 / 27.9</td>\n",
      "<td>70.0</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>     – shuffle spans</td>\n",
      "<td>82.0 / 81.4</td>\n",
      "<td>61.0</td>\n",
      "<td>79.8</td>\n",
      "<td>54.5</td>\n",
      "<td>65.8</td>\n",
      "<td>56.3</td>\n",
      "<td>90.5 / 92.9</td>\n",
      "<td>76.7 / 37.6</td>\n",
      "<td>68.5</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>     + sentinel tokens</td>\n",
      "<td>81.8 / 81.3</td>\n",
      "<td>69.0</td>\n",
      "<td>78.8</td>\n",
      "<td>77.3</td>\n",
      "<td>81.2</td>\n",
      "<td>68.0</td>\n",
      "<td>93.7 / 94.6</td>\n",
      "<td>77.5 / 37.7</td>\n",
      "<td>76.0</td>\n",
      "</tr>\n",
      "</table>\n",
      "</figure>\n",
      "<p>Summary.\n",
      "Above all, we conclude that GLM effectively shares model parameters across natural language understanding and generation tasks, achieving better performance than a standalone BERT, encoder-decoder, or GPT model.</p>\n",
      "</section>\n",
      "<section>\n",
      "<h3>\n",
      "3.4 Ablation Study</h3>\n",
      "<p><a href=\"#S3.T6\">Table 6</a> shows our ablation analysis for GLM. First, to provide an apple-to-apple comparison with BERT, we train a BERT<math></math> model with our implementation, data, and hyperparameters (row 2). The performance is slightly worse than the official BERT<math></math> and significantly worse than GLM<math></math>. It confirms the superiority of GLM over Masked LM pretraining on NLU tasks. Second,\n",
      "we show the SuperGLUE performance of GLM finetuned as sequence classifiers (row 5) and BERT with cloze-style finetuning (row 3). Compared to BERT with cloze-style finetuning, GLM benefits from the autoregressive pretraining. Especially on ReCoRD and WSC, where the verbalizer consists of multiple tokens, GLM consistently outperforms BERT. This demonstrates GLM’s advantage in handling variable-length blank. Another observation is that the cloze formulation is critical for GLM’s performance on NLU tasks. For the large model, cloze-style finetuning can improve the performance by 7 points. Finally, we compare GLM variants with different pretraining designs to understand their importance. Row 6 shows that removing the span shuffling (always predicting the masked spans from left to right) leads to a severe performance drop on SuperGLUE. Row 7 uses different sentinel tokens instead of a single <math></math> token to represent different masked spans. The model performs worse than the standard GLM. We hypothesize that it wastes some modeling capacity to learn the different sentinel tokens which are not used in downstream tasks with only one blank. In <a href=\"#S3.F4\">Figure 4</a>, we show that removing the second dimension of 2D positional encoding hurts the performance of long text generation.</p>\n",
      "<p>We note that T5 is pretrained with a similar blank infilling objective. GLM differs in three aspects: (1) GLM consists of a single encoder, (2) GLM shuffles the masked spans, and (3) GLM uses a single [MASK] instead of multiple sentinel tokens. While we cannot directly compare GLM with T5 due to the differences in training data and the number of parameters, the results in <a href=\"#S2.T1\">Tables 1</a> and <a href=\"#S3.T6\">6</a> have demonstrated the advantage of GLM.</p>\n",
      "</section>\n",
      "</section>\n",
      "<section>\n",
      "<h2>\n",
      "4 Related Work</h2>\n",
      "<p>Pretrained Language Models. Pretraining large-scale language models significantly improves the performance of downstream tasks. There are three types of pretrained models. First, autoencoding models learn a bidirectional contextualized encoder for natural language understanding via denoising objectives <cite>Devlin et al. (<a href=\"#bib.bib9\">2019</a>); Joshi et al. (<a href=\"#bib.bib16\">2020</a>); Yang et al. (<a href=\"#bib.bib46\">2019</a>); Liu et al. (<a href=\"#bib.bib21\">2019</a>); Lan et al. (<a href=\"#bib.bib17\">2020</a>); Clark et al. (<a href=\"#bib.bib6\">2020</a>)</cite>.\n",
      "Second, autoregressive models are trained with a left-to-right language modeling objective <cite>Radford et al. (<a href=\"#bib.bib28\">2018a</a>, <a href=\"#bib.bib29\">b</a>); Brown et al. (<a href=\"#bib.bib4\">2020</a>)</cite>.\n",
      "Third, encoder-decoder models are pretrained for sequence-to-sequence tasks <cite>Song et al. (<a href=\"#bib.bib41\">2019</a>); Lewis et al. (<a href=\"#bib.bib18\">2019</a>); Bi et al. (<a href=\"#bib.bib3\">2020</a>); Zhang et al. (<a href=\"#bib.bib48\">2020</a>)</cite>.</p>\n",
      "<p>Among encoder-decoder models, BART <cite>Lewis et al. (<a href=\"#bib.bib18\">2019</a>)</cite> conducts NLU tasks by feeding the same input into the encoder and decoder, and taking the final hidden states of the decoder. Instead, T5 <cite>Raffel et al. (<a href=\"#bib.bib30\">2020</a>)</cite> formulates most language tasks in the text-to-text framework. However, both models require more parameters to outperform autoencoding models such as RoBERTa <cite>Liu et al. (<a href=\"#bib.bib21\">2019</a>)</cite>.\n",
      "UniLM <cite>Dong et al. (<a href=\"#bib.bib11\">2019</a>); Bao et al. (<a href=\"#bib.bib2\">2020</a>)</cite> unifies three pretraining models under the masked language modeling objective with different attention masks.</p>\n",
      "<p>NLU as Generation. Previously, pretrained language models complete classification tasks for NLU with linear classifiers on the learned representations. GPT-2 <cite>Radford et al. (<a href=\"#bib.bib29\">2018b</a>)</cite> and GPT-3 <cite>Brown et al. (<a href=\"#bib.bib4\">2020</a>)</cite> show that generative language models can complete NLU tasks such as question answering by directly predicting the correct answers without finetuning, given task instructions or a few labeled examples. However, generative models require much more parameters to work due to the limit of unidirectional attention. Recently, PET <cite>Schick and Schütze (<a href=\"#bib.bib35\">2020a</a>, <a href=\"#bib.bib36\">b</a>)</cite> proposes to reformulate input examples as cloze questions with patterns similar to the pretraining corpus in the few-shot setting. It has been shown that combined with gradient-based finetuning, PET can achieve better performance in the few-shot setting than GPT-3 while requiring only 0.1% of its parameters. Similarly, <cite>Athiwaratkun et al. (<a href=\"#bib.bib1\">2020</a>)</cite> and <cite>Paolini et al. (<a href=\"#bib.bib24\">2020</a>)</cite> convert structured prediction tasks, such as sequence tagging and relation extraction, to sequence generation tasks.</p>\n",
      "<p>Blank Language Modeling. <cite>Donahue et al. (<a href=\"#bib.bib10\">2020</a>)</cite> and <cite>Shen et al. (<a href=\"#bib.bib38\">2020</a>)</cite> also study blanking infilling models. Different from their work, we pre-train language models with blank infilling objectives and evaluate their performance in downstream NLU and generation tasks.</p>\n",
      "</section>\n",
      "<section>\n",
      "<h2>\n",
      "5 Conclusions</h2>\n",
      "<p>GLM is a general pretraining framework for natural language understanding and generation. We show that the NLU tasks can be formulated as conditional generation tasks, and therefore solvable by autoregressive models. GLM unifies the pretraining objectives for different tasks as autoregressive blank infilling, with mixed attention masks and the novel 2D position encodings. Empirically we show that GLM outperforms previous methods for NLU tasks and can effectively share parameters for different tasks.</p>\n",
      "</section>\n",
      "<section>\n",
      "<h2>Acknowledgements</h2>\n",
      "<p>The work is supported by the NSFC for Distinguished Young Scholar(61825602), and Beijing Academy of Artificial Intelligence (BAAI).</p>\n",
      "</section>\n",
      "<section>\n",
      "<h2>References</h2>\n",
      "<ul>\n",
      "<li>\n",
      "Athiwaratkun et al. (2020)\n",
      "Ben Athiwaratkun, Cicero dos Santos, Jason Krone, and Bing Xiang. 2020.\n",
      "<a href=\"https://doi.org/10.18653/v1/2020.emnlp-main.27\">Augmented\n",
      "natural language for generative sequence labeling</a>.\n",
      "In <em>Proceedings of the 2020 Conference on Empirical Methods in\n",
      "Natural Language Processing (EMNLP)</em>, pages 375–385.\n",
      "</li>\n",
      "<li>\n",
      "Bao et al. (2020)\n",
      "Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang,\n",
      "Jianfeng Gao, Songhao Piao, Ming Zhou, and Hsiao-Wuen Hon. 2020.\n",
      "<a href=\"http://arxiv.org/abs/2002.12804\">Unilmv2: Pseudo-masked\n",
      "language models for unified language model pre-training</a>.\n",
      "In <em>ICML 2020</em>, volume 119, pages 642–652.\n",
      "</li>\n",
      "<li>\n",
      "Bi et al. (2020)\n",
      "Bin Bi, Chenliang Li, Chen Wu, Ming Yan, Wei Wang, Songfang Huang, Fei Huang,\n",
      "and Luo Si. 2020.\n",
      "<a href=\"https://doi.org/10.18653/v1/2020.emnlp-main.700\">PALM:\n",
      "Pre-training an Autoencoding&amp;Autoregressive Language Model for\n",
      "Context-conditioned Generation</a>.\n",
      "In <em>EMNLP 2020</em>, pages 8681–8691.\n",
      "</li>\n",
      "<li>\n",
      "Brown et al. (2020)\n",
      "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,\n",
      "Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\n",
      "Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom\n",
      "Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens\n",
      "Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott\n",
      "Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\n",
      "Radford, Ilya Sutskever, and Dario Amodei. 2020.\n",
      "<a href=\"https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html\">Language Models are Few-Shot Learners</a>.\n",
      "In <em>NeurIPS 2020</em>.\n",
      "</li>\n",
      "<li>\n",
      "Cer et al. (2017)\n",
      "Daniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-Gazpio, and Lucia\n",
      "Specia. 2017.\n",
      "<a href=\"https://aclanthology.org/S17-2001\">SemEval-2017 Task\n",
      "1: Semantic Textual Similarity Multilingual and Crosslingual Focused\n",
      "Evaluation</a>.\n",
      "In <em>Proceedings of the 11th International Workshop on\n",
      "Semantic Evaluation (SemEval-2017)</em>, pages 1–14.\n",
      "</li>\n",
      "<li>\n",
      "Clark et al. (2020)\n",
      "Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. 2020.\n",
      "<a href=\"https://openreview.net/forum?id=r1xMH1BtvB\">ELECTRA:\n",
      "Pre-training Text Encoders as Discriminators Rather Than\n",
      "Generators</a>.\n",
      "In <em>ICLR 2020</em>.\n",
      "</li>\n",
      "<li>\n",
      "Dagan et al. (2005)\n",
      "Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005.\n",
      "<a href=\"https://doi.org/10.1007/11736790_9\">The pascal recognising\n",
      "textual entailment challenge</a>.\n",
      "In <em>Machine Learning Challenges Workshop</em>, pages 177–190.\n",
      "Springer.\n",
      "</li>\n",
      "<li>\n",
      "Denkowski and Lavie (2014)\n",
      "Michael Denkowski and Alon Lavie. 2014.\n",
      "<a href=\"https://www.aclweb.org/anthology/W14-3348\">Meteor\n",
      "Universal: Language Specific Translation Evaluation for Any Target\n",
      "Language</a>.\n",
      "In <em>Proceedings of the Ninth Workshop on Statistical\n",
      "Machine Translation</em>, pages 376–380.\n",
      "</li>\n",
      "<li>\n",
      "Devlin et al. (2019)\n",
      "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.\n",
      "<a href=\"https://aclanthology.org/N19-1423\">BERT: Pre-training\n",
      "of Deep Bidirectional Transformers for Language Understanding</a>.\n",
      "In <em>NAACL 2019</em>, pages 4171–4186.\n",
      "</li>\n",
      "<li>\n",
      "Donahue et al. (2020)\n",
      "Chris Donahue, Mina Lee, and Percy Liang. 2020.\n",
      "<a href=\"https://doi.org/10.18653/v1/2020.acl-main.225\">Enabling\n",
      "language models to fill in the blanks</a>.\n",
      "pages 2492–2501.\n",
      "</li>\n",
      "<li>\n",
      "Dong et al. (2019)\n",
      "Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao,\n",
      "Ming Zhou, and Hsiao-Wuen Hon. 2019.\n",
      "<a href=\"https://proceedings.neurips.cc/paper/2019/file/c20bb2d9a50d5ac1f713f8b34d9aac5a-Paper.pdf\">Unified language model pre-training for natural language understanding and\n",
      "generation</a>.\n",
      "In <em>NeurIPS 2019</em>, pages 13042–13054.\n",
      "</li>\n",
      "<li>\n",
      "Du et al. (2017)\n",
      "Xinya Du, Junru Shao, and Claire Cardie. 2017.\n",
      "<a href=\"https://doi.org/10.18653/v1/P17-1123\">Learning to Ask:\n",
      "Neural Question Generation for Reading Comprehension</a>.\n",
      "In <em>ACL 2017</em>, pages 1342–1352.\n",
      "</li>\n",
      "<li>\n",
      "Gokaslan and Cohen (2019)\n",
      "Aaron Gokaslan and Vanya Cohen. 2019.\n",
      "Openwebtext corpus.\n",
      "<a href=\"http://Skylion007.github.io/OpenWebTextCorpus\">http://Skylion007.github.io/OpenWebTextCorpus</a>.\n",
      "</li>\n",
      "<li>\n",
      "He et al. (2021)\n",
      "Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2021.\n",
      "<a href=\"http://arxiv.org/abs/2006.03654\">Deberta: Decoding-enhanced\n",
      "bert with disentangled attention</a>.\n",
      "<em>ArXiv</em>, abs/2006.03654.\n",
      "</li>\n",
      "<li>\n",
      "Hendrycks and Gimpel (2016)\n",
      "Dan Hendrycks and Kevin Gimpel. 2016.\n",
      "<a href=\"http://arxiv.org/abs/1606.08415\">Bridging nonlinearities and\n",
      "stochastic regularizers with gaussian error linear units</a>.\n",
      "<em>CoRR</em>, abs/1606.08415.\n",
      "</li>\n",
      "<li>\n",
      "Joshi et al. (2020)\n",
      "Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and\n",
      "Omer Levy. 2020.\n",
      "<a href=\"https://transacl.org/ojs/index.php/tacl/article/view/1853\">SpanBERT: Improving Pre-training by Representing and\n",
      "Predicting Spans</a>.\n",
      "<em>Trans. Assoc. Comput. Linguistics</em>, 8:64–77.\n",
      "</li>\n",
      "<li>\n",
      "Lan et al. (2020)\n",
      "Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and\n",
      "Radu Soricut. 2020.\n",
      "<a href=\"https://openreview.net/forum?id=H1eA7AEtvS\">ALBERT: A\n",
      "Lite BERT for Self-supervised Learning of Language\n",
      "Representations</a>.\n",
      "In <em>ICLR 2020</em>.\n",
      "</li>\n",
      "<li>\n",
      "Lewis et al. (2019)\n",
      "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,\n",
      "Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\n",
      "<a href=\"https://doi.org/10.18653/v1/2020.acl-main.703\">BART:\n",
      "Denoising Sequence-to-Sequence Pre-training for Natural Language\n",
      "Generation, Translation, and Comprehension</a>.\n",
      "In <em>ACL 2020</em>, pages 7871–7880.\n",
      "</li>\n",
      "<li>\n",
      "Lin (2004)\n",
      "Chin-Yew Lin. 2004.\n",
      "<a href=\"https://www.aclweb.org/anthology/W04-1013\">ROUGE: A\n",
      "Package for Automatic Evaluation of Summaries</a>.\n",
      "pages 74–81.\n",
      "</li>\n",
      "<li>\n",
      "Liu and Lapata (2019)\n",
      "Yang Liu and Mirella Lapata. 2019.\n",
      "<a href=\"https://www.aclweb.org/anthology/D19-1387\">Text\n",
      "Summarization with Pretrained Encoders</a>.\n",
      "In <em>EMNLP 2019</em>, pages 3730–3740.\n",
      "</li>\n",
      "<li>\n",
      "Liu et al. (2019)\n",
      "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\n",
      "Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.\n",
      "<a href=\"http://arxiv.org/abs/1907.11692\">Roberta: A robustly\n",
      "optimized BERT pretraining approach</a>.\n",
      "<em>CoRR</em>, abs/1907.11692.\n",
      "</li>\n",
      "<li>\n",
      "Mackenzie et al. (2020)\n",
      "Joel Mackenzie, Rodger Benham, Matthias Petri, Johanne R. Trippas, J. Shane\n",
      "Culpepper, and Alistair Moffat. 2020.\n",
      "<a href=\"https://dl.acm.org/doi/10.1145/3340531.3412762\">CC-News-En: A Large English News Corpus</a>.\n",
      "In <em>CIKM 2020</em>, pages 3077–3084.\n",
      "</li>\n",
      "<li>\n",
      "Narayan et al. (2018)\n",
      "Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018.\n",
      "<a href=\"https://aclanthology.org/D18-1206\">Don’t Give Me the\n",
      "Details, Just the Summary! Topic-Aware Convolutional Neural\n",
      "Networks for Extreme Summarization</a>.\n",
      "In <em>EMNLP 2018</em>, pages 1797–1807.\n",
      "</li>\n",
      "<li>\n",
      "Paolini et al. (2020)\n",
      "Giovanni Paolini, Ben Athiwaratkun, Jason Krone, Jie Ma, Alessandro Achille,\n",
      "Rishita Anubhai, Cicero Nogueira dos Santos, Bing Xiang, and Stefano Soatto.\n",
      "2020.\n",
      "<a href=\"https://openreview.net/forum?id=US-TP-xnXI\">Structured\n",
      "Prediction as Translation between Augmented Natural Languages</a>.\n",
      "</li>\n",
      "<li>\n",
      "Paperno et al. (2016)\n",
      "Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham,\n",
      "Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel\n",
      "Fernández. 2016.\n",
      "<a href=\"https://doi.org/10.18653/v1/p16-1144\">The LAMBADA\n",
      "dataset: Word prediction requiring a broad discourse context</a>.\n",
      "In <em>ACL 2016</em>.\n",
      "</li>\n",
      "<li>\n",
      "Papineni et al. (2002)\n",
      "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.\n",
      "<a href=\"https://www.aclweb.org/anthology/P02-1040\">Bleu: A Method\n",
      "for Automatic Evaluation of Machine Translation</a>.\n",
      "In <em>ACL 2002</em>, pages 311–318.\n",
      "</li>\n",
      "<li>\n",
      "Pereyra et al. (2017)\n",
      "Gabriel Pereyra, George Tucker, Jan Chorowski, Lukasz Kaiser, and Geoffrey E.\n",
      "Hinton. 2017.\n",
      "<a href=\"https://openreview.net/forum?id=HyhbYrGYe\">Regularizing\n",
      "neural networks by penalizing confident output distributions</a>.\n",
      "In <em>5th International Conference on Learning Representations,\n",
      "ICLR 2017, Toulon, France, April 24-26, 2017, Workshop Track Proceedings</em>.\n",
      "</li>\n",
      "<li>\n",
      "Radford et al. (2018a)\n",
      "Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.\n",
      "2018a.\n",
      "<a href=\"https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf\">Improving Language Understanding by Generative Pre-Training</a>.\n",
      "</li>\n",
      "<li>\n",
      "Radford et al. (2018b)\n",
      "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\n",
      "Sutskever. 2018b.\n",
      "<a href=\"https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf\">Language models are unsupervised multitask learners</a>.\n",
      "</li>\n",
      "<li>\n",
      "Raffel et al. (2020)\n",
      "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael\n",
      "Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020.\n",
      "<a href=\"http://arxiv.org/abs/1910.10683\">Exploring the Limits of\n",
      "Transfer Learning with a Unified Text-to-Text Transformer</a>.\n",
      "<em>J. Mach. Learn. Res.</em>, 21:140:1–140:67.\n",
      "</li>\n",
      "<li>\n",
      "Rajpurkar et al. (2018)\n",
      "Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\n",
      "<a href=\"https://aclanthology.org/P18-2124\">Know What You Don’t\n",
      "Know: Unanswerable Questions for SQuAD</a>.\n",
      "In <em>ACL 2018</em>, pages 784–789.\n",
      "</li>\n",
      "<li>\n",
      "Rajpurkar et al. (2016)\n",
      "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016.\n",
      "<a href=\"https://doi.org/10.18653/v1/d16-1264\">Squad: 100, 000+\n",
      "questions for machine comprehension of text</a>.\n",
      "In <em>EMNLP 2016</em>, pages 2383–2392.\n",
      "</li>\n",
      "<li>\n",
      "Rasley et al. (2020)\n",
      "Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020.\n",
      "<a href=\"https://doi.org/10.1145/3394486.3406703\">Deepspeed: System\n",
      "optimizations enable training deep learning models with over 100 billion\n",
      "parameters</a>.\n",
      "In <em>KDD 2020</em>, pages 3505–3506.\n",
      "</li>\n",
      "<li>\n",
      "Rush et al. (2015)\n",
      "Alexander M. Rush, Sumit Chopra, and Jason Weston. 2015.\n",
      "<a href=\"https://aclanthology.org/D15-1044\">A neural attention model\n",
      "for abstractive sentence summarization</a>.\n",
      "In <em>EMNLP 2015</em>, pages 379–389.\n",
      "</li>\n",
      "<li>\n",
      "Schick and Schütze (2020a)\n",
      "Timo Schick and Hinrich Schütze. 2020a.\n",
      "<a href=\"https://aclanthology.org/2021.eacl-main.20/\">Exploiting\n",
      "Cloze Questions for Few Shot Text Classification and Natural\n",
      "Language Inference</a>.\n",
      "pages 255–269.\n",
      "</li>\n",
      "<li>\n",
      "Schick and Schütze (2020b)\n",
      "Timo Schick and Hinrich Schütze. 2020b.\n",
      "<a href=\"https://doi.org/10.18653/v1/2021.naacl-main.185\">It’s Not\n",
      "Just Size That Matters: Small Language Models Are Also Few-Shot\n",
      "Learners</a>.\n",
      "pages 2339–2352.\n",
      "</li>\n",
      "<li>\n",
      "See et al. (2017)\n",
      "Abigail See, Peter J. Liu, and Christopher D. Manning. 2017.\n",
      "<a href=\"https://doi.org/10.18653/v1/P17-1099\">Get To The Point:\n",
      "Summarization with Pointer-Generator Networks</a>.\n",
      "In <em>ACL 2017</em>, pages 1073–1083.\n",
      "</li>\n",
      "<li>\n",
      "Shen et al. (2020)\n",
      "Tianxiao Shen, Victor Quach, Regina Barzilay, and Tommi S. Jaakkola. 2020.\n",
      "<a href=\"https://doi.org/10.18653/v1/2020.emnlp-main.420\">Blank\n",
      "language models</a>.\n",
      "pages 5186–5198.\n",
      "</li>\n",
      "<li>\n",
      "Shoeybi et al. (2019)\n",
      "Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,\n",
      "and Bryan Catanzaro. 2019.\n",
      "<a href=\"http://arxiv.org/abs/1909.08053\">Megatron-lm: Training\n",
      "multi-billion parameter language models using model parallelism</a>.\n",
      "<em>CoRR</em>, abs/1909.08053.\n",
      "</li>\n",
      "<li>\n",
      "Socher et al. (2013)\n",
      "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning,\n",
      "Andrew Ng, and Christopher Potts. 2013.\n",
      "<a href=\"https://aclanthology.org/D13-1170\">Recursive Deep Models\n",
      "for Semantic Compositionality Over a Sentiment Treebank</a>.\n",
      "In <em>EMNLP 2013</em>, pages 1631–1642.\n",
      "</li>\n",
      "<li>\n",
      "Song et al. (2019)\n",
      "Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. 2019.\n",
      "<a href=\"http://proceedings.mlr.press/v97/song19d.html\">MASS:\n",
      "Masked Sequence to Sequence Pre-training for Language\n",
      "Generation</a>.\n",
      "In <em>ICML 2019</em>, volume 97, pages 5926–5936.\n",
      "</li>\n",
      "<li>\n",
      "Trinh and Le (2019)\n",
      "Trieu H. Trinh and Quoc V. Le. 2019.\n",
      "<a href=\"http://arxiv.org/abs/1806.02847\">A Simple Method for\n",
      "Commonsense Reasoning</a>.\n",
      "<em>arXiv:1806.02847 [cs]</em>.\n",
      "</li>\n",
      "<li>\n",
      "Wang et al. (2019)\n",
      "Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael,\n",
      "Felix Hill, Omer Levy, and Samuel R. Bowman. 2019.\n",
      "<a href=\"https://proceedings.neurips.cc/paper/2019/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html\">SuperGLUE: A Stickier Benchmark for General-Purpose Language\n",
      "Understanding Systems</a>.\n",
      "In <em>NeurIPS 2019</em>, pages 3261–3275.\n",
      "</li>\n",
      "<li>\n",
      "Wang et al. (2018)\n",
      "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel\n",
      "Bowman. 2018.\n",
      "<a href=\"https://openreview.net/forum?id=rJ4km2R5t7\">GLUE: A\n",
      "Multi-Task Benchmark and Analysis Platform for Natural Language\n",
      "Understanding</a>.\n",
      "In <em>ICLR 2019</em>, pages 353–355.\n",
      "</li>\n",
      "<li>\n",
      "Williams et al. (2018)\n",
      "Adina Williams, Nikita Nangia, and Samuel Bowman. 2018.\n",
      "<a href=\"https://doi.org/10.18653/v1/n18-1101\">A Broad-Coverage\n",
      "Challenge Corpus for Sentence Understanding through Inference</a>.\n",
      "In <em>NAACL 2018</em>, pages 1112–1122.\n",
      "</li>\n",
      "<li>\n",
      "Yang et al. (2019)\n",
      "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov,\n",
      "and Quoc V. Le. 2019.\n",
      "<a href=\"https://papers.nips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf\">XLNet: Generalized Autoregressive Pretraining for Language\n",
      "Understanding</a>.\n",
      "In <em>NeurIPS 2019</em>, pages 5754–5764.\n",
      "</li>\n",
      "<li>\n",
      "Yang et al. (2017)\n",
      "Zichao Yang, Zhiting Hu, Ruslan Salakhutdinov, and Taylor Berg-Kirkpatrick.\n",
      "2017.\n",
      "<a href=\"http://proceedings.mlr.press/v70/yang17d.html\">Improved\n",
      "variational autoencoders for text modeling using dilated convolutions</a>.\n",
      "In <em>ICML 2017</em>, volume 70, pages 3881–3890.\n",
      "</li>\n",
      "<li>\n",
      "Zhang et al. (2020)\n",
      "Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. 2020.\n",
      "<a href=\"http://proceedings.mlr.press/v119/zhang20ae.html\">PEGASUS: Pre-training with Extracted Gap-sentences for\n",
      "Abstractive Summarization</a>.\n",
      "In <em>ICML 2020</em>, pages 11328–11339.\n",
      "</li>\n",
      "<li>\n",
      "Zhu et al. (2019)\n",
      "Wanrong Zhu, Zhiting Hu, and Eric Xing. 2019.\n",
      "<a href=\"http://arxiv.org/abs/1901.00158\">Text infilling</a>.\n",
      "<em>arXiv preprint arXiv:1901.00158</em>.\n",
      "</li>\n",
      "<li>\n",
      "Zhu et al. (2015)\n",
      "Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun,\n",
      "Antonio Torralba, and Sanja Fidler. 2015.\n",
      "<a href=\"https://doi.org/10.1109/ICCV.2015.11\">Aligning books and\n",
      "movies: Towards story-like visual explanations by watching movies and reading\n",
      "books</a>.\n",
      "In <em>ICCV 2015</em>, pages 19–27.\n",
      "</li>\n",
      "</ul>\n",
      "</section>\n",
      "<section>\n",
      "<h2>\n",
      "Appendix A Pretraining Setting</h2>\n",
      "<section>\n",
      "<h3>\n",
      "A.1 Datasets</h3>\n",
      "<p>To train GLM<math></math> and GLM<math></math>, we use BookCorpus <cite>Zhu et al. (<a href=\"#bib.bib50\">2015</a>)</cite> and Wikipedia used by BERT <cite>Devlin et al. (<a href=\"#bib.bib9\">2019</a>)</cite>.</p>\n",
      "<p>To train GLM<math></math>, we follow the pretraining datasets of RoBERTa <cite>Liu et al. (<a href=\"#bib.bib21\">2019</a>)</cite>, which consist of BookCorups <cite>Zhu et al. (<a href=\"#bib.bib50\">2015</a>)</cite>,Wikipedia (16GB), CC-News (the English portion of the CommonCrawl News dataset<sup>3</sup><sup>3</sup>3<a href=\"https://commoncrawl.org/2016/10/news-dataset-available\">https://commoncrawl.org/2016/10/news-dataset-available</a> 76GB), OpenWebText (web content extracted from URLs shared on Reddit with at least three upvotes<cite>Gokaslan and Cohen (<a href=\"#bib.bib13\">2019</a>)</cite>, 38GB) and Stories (subset of CommonCrawl data filtered to match the story-like style of Winograd schemas <cite>Trinh and Le (<a href=\"#bib.bib42\">2019</a>)</cite>, 31GB). The Stories dataset is no longer publicly available<sup>4</sup><sup>4</sup>4<a href=\"https://github.com/tensorflow/models/tree/archive/research/lm_commonsense#1-download-data-files\">https://github.com/tensorflow/models/tree/archive/research/lm_commonsense#1-download-data-files</a>. Therefore, we remove the Stories dataset and replace OpenWebText with OpenWebText2<sup>5</sup><sup>5</sup>5<a href=\"https://openwebtext2.readthedocs.io/en/latest\">https://openwebtext2.readthedocs.io/en/latest</a> (66GB). The CC-News dataset is not publicly available and we use the CC-News-en published by <cite>Mackenzie et al. (<a href=\"#bib.bib22\">2020</a>)</cite>. All the datasets used total 158GB of uncompressed texts, close in size to RoBERTa’s 160GB datasets.</p>\n",
      "</section>\n",
      "<section>\n",
      "<h3>\n",
      "A.2 Hyperparameters</h3>\n",
      "<figure>\n",
      "<figcaption>Table 7: Hyperparameters for pretraining</figcaption>\n",
      "<table>\n",
      "<tr>\n",
      "<td>Hyperparameters</td>\n",
      "<td>GLM <math></math>\n",
      "</td>\n",
      "<td>GLM <math></math>\n",
      "</td>\n",
      "<td>GLM <math></math>\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Number of Layers</td>\n",
      "<td>12</td>\n",
      "<td>24</td>\n",
      "<td>24</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Hidden size</td>\n",
      "<td>768</td>\n",
      "<td>1024</td>\n",
      "<td>1024</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>FFN inner hidden size</td>\n",
      "<td>3072</td>\n",
      "<td>4096</td>\n",
      "<td>4096</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Attention heads</td>\n",
      "<td>12</td>\n",
      "<td>16</td>\n",
      "<td>16</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Attention head size</td>\n",
      "<td>64</td>\n",
      "<td>64</td>\n",
      "<td>64</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Dropout</td>\n",
      "<td>0.1</td>\n",
      "<td>0.1</td>\n",
      "<td>0.1</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Attention Dropout</td>\n",
      "<td>0.1</td>\n",
      "<td>0.1</td>\n",
      "<td>0.1</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Warmup Steps</td>\n",
      "<td>6k</td>\n",
      "<td>8k</td>\n",
      "<td>30K</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Peak Learning Rate</td>\n",
      "<td>4e-4</td>\n",
      "<td>2e-4</td>\n",
      "<td>4e-4</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Batch Size</td>\n",
      "<td>1024</td>\n",
      "<td>1024</td>\n",
      "<td>8192</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Weight Decay</td>\n",
      "<td>0.1</td>\n",
      "<td>0.1</td>\n",
      "<td>0.01</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Max Steps</td>\n",
      "<td>120k</td>\n",
      "<td>200k</td>\n",
      "<td>250k</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Learning Rate Decay</td>\n",
      "<td>Cosine</td>\n",
      "<td>Cosine</td>\n",
      "<td>Cosine</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Adam <math></math>\n",
      "</td>\n",
      "<td>1e-6</td>\n",
      "<td>1e-6</td>\n",
      "<td>1e-6</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Adam <math></math>\n",
      "</td>\n",
      "<td>0.9</td>\n",
      "<td>0.9</td>\n",
      "<td>0.9</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Adam <math></math>\n",
      "</td>\n",
      "<td>0.98</td>\n",
      "<td>0.98</td>\n",
      "<td>0.98</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Gradient Clipping</td>\n",
      "<td>1.0</td>\n",
      "<td>1.0</td>\n",
      "<td>1.0</td>\n",
      "</tr>\n",
      "</table>\n",
      "</figure>\n",
      "<p>The hyperparameters for GLM<math></math> and GLM<math></math> are similar to those used by BERT. For trade-off of training speed and fair comparison with BERT (batch size 256 and 1,000,000 training steps), we use batch size of 1024 and 200,000 training steps for GLM<math></math>. Since GLM<math></math> is smaller, we reduce the number of training steps to 120,000 to speed up pre-training. The hyperparameters for GLM<math></math> and GLM<math></math> are the same as those of GLM<math></math>. The hyperparameters except Transformer architecture for GLM<math></math> and GLM<math></math> are the same as those of GLM<math></math>. The models are trained on 64 V100 GPUs for 200K steps with batch size of 1024 and maximum sequence length of 512, which takes about 2.5 days for GLM<math></math>.</p>\n",
      "<p>To train GLM<math></math>, we follow most of the hyperparameters of RoBERTa. The main difference includes: (1) Due to resource limit, we only pre-train GLM <math></math> for 250,000 steps, which are half of RoBERTa and BART’s training steps, and close to T5 in number of trained tokens. (2) We use cosine decay instead of linear decay for learning rate scheduling (3) We additionally apply gradient clipping with value 1.0.</p>\n",
      "<p>The hyperparameters for all the pre-training settings are summarized in <a href=\"#A1.T7\">Table 7</a>.</p>\n",
      "</section>\n",
      "<section>\n",
      "<h3>\n",
      "A.3 Implementation</h3>\n",
      "<p>Our pretraining implementation is based on Megatron-LM <cite>Shoeybi et al. (<a href=\"#bib.bib39\">2019</a>)</cite> and DeepSpeed <cite>Rasley et al. (<a href=\"#bib.bib33\">2020</a>)</cite>. We include our code in the supplementary material. Due to the size limit of supplementary material, we cannot include the pretrained models, but will make them public available in the future.</p>\n",
      "</section>\n",
      "</section>\n",
      "<section>\n",
      "<h2>\n",
      "Appendix B Downstream Tasks</h2>\n",
      "<section>\n",
      "<h3>\n",
      "B.1 SuperGLUE</h3>\n",
      "<p>The SuperGLUE benchmark consists of 8 NLU tasks. We formulate them as blank infilling tasks, following <cite>Schick and Schütze (<a href=\"#bib.bib36\">2020b</a>)</cite>. Table <a href=\"#A2.T8\">8</a> shows the cloze questions and verbalizers we used in our experiments. For 3 tasks (ReCoRD, COPA, and WSC), the answer may consist of multiple tokens, and for the other 5 tasks, the answer is always a single token.</p>\n",
      "<p>When finetuning GLM on the SuperGLUE tasks, we construct the input using the cloze questions in Table <a href=\"#A2.T8\">8</a> and replace the blank with a [MASK] token. Then we compute the score of generating each answer candidate. For the 5 single-token tasks, the score is defined to be the logit of the verbalizer token. For the 3 multi-token tasks, we use the sum of the log-probabilities of the verbalizer tokens. Thanks to the autoregressive blank infilling mechanism we proposed, we can obtain all the log-probabilities in one pass. Then we compute the cross entropy loss using the groundtruth label and update the model parameters.</p>\n",
      "<p>For the baseline classifiers, we follow the standard practice to concatenate the input parts of each task (such as the premise and hypothesis for textual entailment, or the passage, question and answer for ReCORD and MultiRC) and add a classification layer on top of the [CLS] token representation. We also implemented cloze-style finetuning for the other pre-trained models, but the performance was usually similar to the standard classifier, as we shown in the ablation study. Models with blank-infilling objectives, such as T5 and our GLM, benefits more from converting the NLU tasks into cloze questions. Thus for T5 and GLM, we report the performance after such conversion in our main results.</p>\n",
      "<figure>\n",
      "<figcaption>Table 8: Cloze questions and verbalizers for the 8 SuperGLUE tasks used in our experiments. <sup>∗</sup> denotes the answer contains multiple tokens.</figcaption>\n",
      "<table>\n",
      "<tr>\n",
      "<td>Dataset</td>\n",
      "<td>Task</td>\n",
      "<td>\n",
      "Cloze Question\n",
      "</td>\n",
      "<td>\n",
      "Verbalizers\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>ReCoRD<sup>∗</sup>\n",
      "</td>\n",
      "<td>Question answering</td>\n",
      "<td>\n",
      "[passage <math></math>] [cloze question <math></math>]\n",
      "</td>\n",
      "<td>\n",
      "Answer candidates\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>COPA<sup>∗</sup>\n",
      "</td>\n",
      "<td>Causal reasoning</td>\n",
      "<td>\n",
      "“[choice <math></math>]” or “[choice <math></math>]”? [premise <math></math>], so  .\n",
      "</td>\n",
      "<td>\n",
      "<math></math> / <math></math>\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>WSC<sup>∗</sup>\n",
      "</td>\n",
      "<td>Coreference resolution</td>\n",
      "<td>\n",
      "[sentence <math></math>] The pronoun ‘<math></math>’ refers to  .\n",
      "</td>\n",
      "<td>\n",
      "Noun <math></math>\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>RTE</td>\n",
      "<td>Textual entailment</td>\n",
      "<td>\n",
      "“[hypothesis <math></math>]”? <math></math>  , “[premise <math></math>]”\n",
      "</td>\n",
      "<td>\n",
      "“yes” (entailment), “no” (not entailment)\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>BoolQ</td>\n",
      "<td>Question answering</td>\n",
      "<td>\n",
      "[passage <math></math>]. Question: <math></math>? Answer:  .\n",
      "</td>\n",
      "<td>\n",
      "“yes” / “no”\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>WiC</td>\n",
      "<td>Word sense disambiguation</td>\n",
      "<td>\n",
      "“[sentence <math></math>]” / “[sentence <math></math>]” Similar sense of [word <math></math>]?  .\n",
      "</td>\n",
      "<td>\n",
      "“yes” / “no”\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>CB</td>\n",
      "<td>Textual entailment</td>\n",
      "<td>\n",
      "“[hypothesis <math></math>]”? <math></math>  , “[premise <math></math>]”\n",
      "</td>\n",
      "<td>\n",
      "“yes” (entailment), “no” (contradiction), “maybe” (neutral)\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>MultiRC</td>\n",
      "<td>Question answering</td>\n",
      "<td>\n",
      "[passage <math></math>]. Question: <math></math>? Is it [answer <math></math>]?  .\n",
      "</td>\n",
      "<td>\n",
      "“yes” / “no”\n",
      "</td>\n",
      "</tr>\n",
      "</table>\n",
      "</figure>\n",
      "</section>\n",
      "<section>\n",
      "<h3>\n",
      "B.2 Sequence-to-Sequence</h3>\n",
      "<p>Fot the text summarization task, we use the dataset Gigaword <cite>Rush et al. (<a href=\"#bib.bib34\">2015</a>)</cite> for model fine-tuning and evaluation. We finetune GLM<math></math> on the training set for 4 epochs with AdamW optimizer. The learning rate has a peak value of 3e-5, warm-up over the 6% training steps and a linear decay. We also use label smoothing with rate 0.1 <cite>Pereyra et al. (<a href=\"#bib.bib27\">2017</a>)</cite>. The maximum document length is 192 and the maximum summary length is 32. During decoding, we use beam search with beam size of 5 and remove repeated trigrams. We tweak the value of length penalty on the development set. The evaluation metrics are the F1 scores of Rouge-1, Rouge-2, and Rouge-L <cite>Lin (<a href=\"#bib.bib19\">2004</a>)</cite> on the test set.</p>\n",
      "<p>For the question generation task, we use the SQuAD 1.1 dataset <cite>Rajpurkar et al. (<a href=\"#bib.bib32\">2016</a>)</cite> and follow the dataset split of <cite>Du et al. (<a href=\"#bib.bib12\">2017</a>)</cite>. The optimizer hyperparameters are the same as those of abstractive summarization. The maximum passage length is 464 and the maximum question length is 48. During decoding, we use beam search with beam size 5 and tweak the value of length penalty on the development set. The evaluation metrics are the scores of BLEU-1, BLEU-2, BLEU-3, BLEU-4 <cite>Papineni et al. (<a href=\"#bib.bib26\">2002</a>)</cite>, METEOR <cite>Denkowski and Lavie (<a href=\"#bib.bib8\">2014</a>)</cite> and Rouge-L <cite>Lin (<a href=\"#bib.bib19\">2004</a>)</cite>.</p>\n",
      "<p>Results of T5<math></math> on XSum are obtained by running the summarization script provided by Huggingface transformers<sup>6</sup><sup>6</sup>6<a href=\"https://github.com/huggingface/transformers/tree/master/examples/pytorch/summarization\">https://github.com/huggingface/transformers/tree/master/examples/pytorch/summarization</a>. All the other results of baselines on seq2seq tasks are obtained from the corresponding papers.</p>\n",
      "</section>\n",
      "<section>\n",
      "<h3>\n",
      "B.3 Text Infilling</h3>\n",
      "<p>We follow <cite>Shen et al. (<a href=\"#bib.bib38\">2020</a>)</cite> and evaluate text infilling performance on the Yahoo Answers dataset <cite>Yang et al. (<a href=\"#bib.bib47\">2017</a>)</cite>, which contains 100K/10K/10K documents for train/valid/test respectively. The average document length is 78 words. To construct the text infilling task, we randomly mask a given ratio <math></math> of each document’s tokens and the contiguous masked tokens are collapsed into a single blank. We finetune GLM<math></math> on the training set for 5 epochs with dynamic masking, i.e. the blanks are randomly generated at training time. Similar to the sequence-to-sequence experiments, we use an AdamW optimizer with a peak learning rate 1e-5 and 6% warm-up linear scheduler.</p>\n",
      "<p>For comparison with previous work, we use the same test set constructed by <cite>Shen et al. (<a href=\"#bib.bib38\">2020</a>)</cite>. The evaluation metric is the BLEU score of the infilled text against the original document. We compare with two baselines: (1) BERT, which learns a left-to-right language model to generate the masked tokens on top of the blank representation, and (2) BLM proposed by <cite>Shen et al. (<a href=\"#bib.bib38\">2020</a>)</cite>, which can fill in the blank with arbitrary trajectories.</p>\n",
      "</section>\n",
      "<section>\n",
      "<h3>\n",
      "B.4 Language Modeling</h3>\n",
      "<p>We evaluate the model’s ability of language modeling with perplexity on BookWiki and accuracy on the LAMBDA dataset <cite>Paperno et al. (<a href=\"#bib.bib25\">2016</a>)</cite>.</p>\n",
      "<p>Perplexity is an evaluation criterion that has been well studied for language modeling. Perplexity is the exponentiation of the average cross entropy of a corpus.</p>\n",
      "<table>\n",
      "<tbody><tr>\n",
      "<td></td>\n",
      "<td><math></math></td>\n",
      "<td></td>\n",
      "<td>(4)</td>\n",
      "</tr></tbody>\n",
      "</table>\n",
      "<p>where <math></math>. Since transformers can only operate on a window of fixed input size <math></math>, we cannot fully calculate <math></math> and can only calculate <math></math>. Even calculating this value for each token is prohibitively expensive, since we need to conduct <math></math> evaluations of <math></math>-size contexts. To improve evaluation efficiency, we adopt <em>overlapping evaluation</em>, where we advance the sliding windows by some overlap <math></math> each time and only compute the cross entropy loss for the last <math></math> tokens of the window. In our experiments we set <math></math> for all the models.</p>\n",
      "<p>LAMBDA is a cloze-style dataset to test the ability of long-range dependency modeling. Each example is a passage consisting of 4-5 sentences with the last word missing and the model is required to predict the last word of the passage. Since we use WordPiece tokenization, a word can be split into several subword units. We use teacher forcing and consider the prediction correct only when all the predicted tokens are correct.</p>\n",
      "</section>\n",
      "</section>\n",
      "<section>\n",
      "<h2>\n",
      "Appendix C Results on Other NLU Benchmarks</h2>\n",
      "<p>GLUE <cite>Wang et al. (<a href=\"#bib.bib44\">2018</a>)</cite> is another widely-used NLU benchmark, including single sentence tasks (e.g. sentiment analysis <cite>Socher et al. (<a href=\"#bib.bib40\">2013</a>)</cite>) and sentence pair tasks (e.g. text similarity <cite>Cer et al. (<a href=\"#bib.bib5\">2017</a>)</cite> and natural language inference <cite>Williams et al. (<a href=\"#bib.bib45\">2018</a>); Dagan et al. (<a href=\"#bib.bib7\">2005</a>)</cite>). The benchmark is usually considered as less challenging than SuperGLUE. SQuAD <cite>Rajpurkar et al. (<a href=\"#bib.bib32\">2016</a>, <a href=\"#bib.bib31\">2018</a>)</cite> is an extractive question answering benchmark. We further compare GLM with BERT on the two benchmarks.</p>\n",
      "<p>The results on GLUE and SQuAD are shown in <a href=\"#A3.T9\">Tables 9</a> and <a href=\"#A3.T10\">10</a>. On the two benchmarks, GLM can still outperform BERT with the same amount of parameters, but with a smaller margin.</p>\n",
      "<figure>\n",
      "<figcaption>Table 9: Results on the GLUE dev set.</figcaption>\n",
      "<table>\n",
      "<tr>\n",
      "<td>Model</td>\n",
      "<td>MNLI</td>\n",
      "<td>QNLI</td>\n",
      "<td>QQP</td>\n",
      "<td>RTE</td>\n",
      "<td>SST-2</td>\n",
      "<td>MRPC</td>\n",
      "<td>CoLA</td>\n",
      "<td>STS-B</td>\n",
      "<td>Avg</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>BERT<math></math>\n",
      "</td>\n",
      "<td>86.6</td>\n",
      "<td>92.3</td>\n",
      "<td>91.3</td>\n",
      "<td>73.6</td>\n",
      "<td>93.2</td>\n",
      "<td>88.0</td>\n",
      "<td>60.6</td>\n",
      "<td>90.0</td>\n",
      "<td>84.4</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>GLM<math></math>\n",
      "</td>\n",
      "<td>86.7</td>\n",
      "<td>92.8</td>\n",
      "<td>91.5</td>\n",
      "<td>74.0</td>\n",
      "<td>93.5</td>\n",
      "<td>90.0</td>\n",
      "<td>61.4</td>\n",
      "<td>90.7</td>\n",
      "<td>85.1</td>\n",
      "</tr>\n",
      "</table>\n",
      "</figure>\n",
      "<figure>\n",
      "<figcaption>Table 10: Results on the SQuAD v1.1/v2.0 dev sets.</figcaption>\n",
      "<table>\n",
      "<tr>\n",
      "<td>Model</td>\n",
      "<td>\n",
      " \n",
      "SQuAD v1.1\n",
      "EM/F1\n",
      "</td>\n",
      "<td>\n",
      " \n",
      "SQuAD v2.0\n",
      "EM/F1\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>BERT<math></math>\n",
      "</td>\n",
      "<td>80.8 / 88.5</td>\n",
      "<td>73.7 / 76.3</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>GLM<math></math>\n",
      "</td>\n",
      "<td>\n",
      "81.5 / 88.6\n",
      "</td>\n",
      "<td>\n",
      "74.7 / 77.8\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>BERT<math></math>\n",
      "</td>\n",
      "<td>84.1 / 90.9</td>\n",
      "<td>79.0 / 81.8</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>GLM<math></math>\n",
      "</td>\n",
      "<td>\n",
      "85.4 / 91.6\n",
      "</td>\n",
      "<td>\n",
      "80.3 / 83.3\n",
      "</td>\n",
      "</tr>\n",
      "</table>\n",
      "</figure>\n",
      "</section>\n",
      "<section>\n",
      "<h2>\n",
      "Appendix D Text Generation Samples</h2>\n",
      "<p>We show texts generated by GLM<math></math> given unseen contexts randomly sampled from the test set. We use top-<math></math> random sampling with <math></math> for generation and set maximum sequence length to 512. Some of the texts are cut short.</p>\n",
      "<h6>\n",
      "Example D.1.\n",
      "</h6>\n",
      "<p>The Wyoming State Legislature is the legislative branch of the U.S. State of Wyoming.</p>\n",
      "<p>GLM: It consists of a state senate and a house of representatives. As of 2019, state senators are Steve Kish, Joe L. Williams, and Linda S. Bollens. Members of the Wyoming State Legislature are elected from single-member districts representing the majority of the state. The current state senate members are: In recent years, there have been four changes to the senate. The most recent is the creation of a six-seat district that includes all or part of the following: In the 2009 elections, the state senate members were elected to six-year terms. The current state house members are: The Wyoming Constitution assigns certain powers to the governor. Most notably, the governor is president of the senate and governor. However, if the governor desires to appoint a member to the Wyoming state senate, a law authorizes the governor to do so. The governor of Wyoming holds no legislative power but has the power to veto lawmakers, which is not limited to the veto of laws. Under the wyoming state constitution, the governor can veto the actions of the other members of the wyoming house of representatives. The governor can also appoint members of the wyoming senate. In addition, the governor can appoint members of the Wyoming house of representatives. Wyoming’s constitution provides that the governor can appoint a member of the wyoming state senate to the wyoming supreme court, and the chairman of the wyoming senate.</p>\n",
      "<h6>\n",
      "Example D.2.\n",
      "</h6>\n",
      "<p>Jonathan Terry is a television and film actor.</p>\n",
      "<p>GLM: He was a voice actor for the “X-Men\" cartoon series. He was also a voice actor for “the Simpsons\" as well as “the marvelous misadventures of superman. He also has voiced characters in “the legend of korra\" He has appeared on several television series, including “the simpsons\", “the x-files\", “heroes\" and “the simpsons movie\" as the character captain billy higgledy-pig, and the web series “krusty mysteries\" as the character Colonel Trungus. He wrote for “the Guardian\" newspaper. Jonathan Terry was born in London. Terry grew up in Surrey, England and attended the University of Sussex in the United Kingdom, graduating with a degree in english literature. He was a guest lecturer at King’s College London, and then took two years of acting courses at the brit school of acting to prepare for his future career in the entertainment industry. Terry first appeared in the TV series “the Simpsons\" as the character captain Billy Higgledy-pig, but his character was only a one-time recurring character in the series’ first six seasons. He later appeared as a regular for the show’s final six seasons, and has been a frequent guest in the show since. He appeared in the first few episodes of “\" as the character major Jack Ryan. He has also appeared as part of the supporting cast of several episodes of “the secret life of pets\". He has also worked on “the simpsons\" TV show since “the simpsons movie\", most notably playing the roles of Captain Skeletor and the ghost of the same name. He plays characters in several films, including “\", “\", “\" and “\". He has appeared in music videos for the killers in 1993, the pretenders in 1995, and in the TV shows “the royal\" and “the bill\".</p>\n",
      "<h6>\n",
      "Example D.3.\n",
      "</h6>\n",
      "<p>Corona was a station along the port Washington branch of the long island rail road in the Corona section of queens, New York City. It was one of two stations built by the flushing railroad in Corona, this one having been at Grand Avenue (later called National Avenue, now National Street ) and 45th Avenue.</p>\n",
      "<p>GLM: It had a brick entrance building next to the tracks, and one platform with 2 side platforms. The station burned down and was rebuilt in 1908. The station closed on september 15, 1927, with the train service transferred from Grand Avenue to 45th Avenue. There were plans to build a subway extension to Corona, but it was never built. In the 1980s, two stations were constructed on the line, Corona Road and Corona Park. The new Corona station opened in 1988, and the original Corona station was demolished.</p>\n",
      "<h6>\n",
      "Example D.4.\n",
      "</h6>\n",
      "<p>Robert Lee Smith (born July 5, 1938) is a former American football cornerback in the national football league. He played for the Los Angeles Rams (1962–1965) and the Detroit Lions (1965–1966). He was also a kick and punt returner.</p>\n",
      "<p>GLM: In his four-year NFL career, he played in 33 games and started 14, registering 62 career interceptions. He completed his NFL career with five interceptions. Smith was born in La Canada Flintridge, Michigan, in 1938. He attended Bishop O’ Dowd high school in Flintridge. Smith’ s father was a pharmacist. He graduated from Michigan State University in 1958 with a degree in business administration. Smith played for the Michigan Wolverines football team from 1959 to 1963. In his two years as a reserve cornerback, he led the conference in interceptions with five. In 1962, he set the Wolverines’ all-time interception record with 13, and was second overall in the 1962 season’s Heisman Trophy voting. Smith also won the Dick Butkus award as the nation’s outstanding linebacker. In 1961, the “Los Angeles Times\" wrote that Smith “is an outstanding pass rusher, with an average of almost 100 yards per punt return.\" Smith was inducted into the university of Michigan athletic hall of honor in 1989 and the national football foundation hall of fame in 1991. He was elected to the Michigan sports hall of fame in 1995. Smith earned the honor because of his accomplishments prior to his NFL career. He was one of four Michigan players honored as first-overall selections in the 1964 NFL draft. The others were Joe Namath, Bill Nelsen, and Jerry Kramer. In 1966, the NFL gave players $300,000 a season to play football. After his rookie season, he was not selected to play in the 1966 pro bowl. On January 13, 1966, the Rams traded smith to the Detroit Lions for Paul Hornung, and later that year he was traded to the Lions for Ray “the Lion\" Jones in exchange for Linebacker Jim “the Hawk\" Johnson. On September 10, 1968, he was traded back to Los Angeles for a second round pick in the 1970 draft. He was also traded to the St. Louis Cardinals for a second round pick in the 1970 draft. On June 2, 1970 he was cut by the Cardinals. On November 15, 1970, the Los Angeles Rams acquired Smith from the Lions in exchange for Linebacker Tony Harris. The Rams waived Smith during the September 1, 1972 offseason. Smith’s number at Michigan State was # 7 in 1969.</p>\n",
      "</section>\n",
      "<a href=\"/html/2103.10359\">◄</a>\n",
      "<a href=\"/\"><img src=\"/assets/ar5iv.png\"/></a>\n",
      "<a href=\"/feeling_lucky\">Feeling<br/>lucky?</a>\n",
      "<a href=\"/log/2103.10360\">Conversion<br/>report</a>\n",
      "<a href=\"https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2103.10360\">Report<br/>an issue</a>\n",
      "<a href=\"https://arxiv.org/abs/2103.10360\">View original<br/>on arXiv</a><a href=\"/html/2103.10361\">►</a>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def fetch_and_clean_html(url):\n",
    "    # Fetch the HTML content from the URL\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Ensure the request was successful\n",
    "    html_content = response.text\n",
    "    \n",
    "    # Parse the HTML using BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Remove <head>, <annotation-xml>, and <semantics> tags (including their content)\n",
    "    for tag in soup(['head', 'annotation-xml', 'semantics', 'footer', 'script']):\n",
    "        tag.decompose()\n",
    "    \n",
    "    # Remove <div> and <span> tags but keep their content\n",
    "    for tag in soup.find_all(['div', 'span', '!DOCTYPE html', 'article', 'html', 'body']):\n",
    "        # Unwrap the tag (replace the tag with its content)\n",
    "        tag.unwrap()\n",
    "    \n",
    "    # Remove all attributes except for 'href' in <a> and 'src' in <img>\n",
    "    for tag in soup.find_all(True):  # True finds all tags\n",
    "        if tag.name == 'a':\n",
    "            attrs_to_keep = {'href'}\n",
    "        elif tag.name == 'img':\n",
    "            attrs_to_keep = {'src'}\n",
    "        else:\n",
    "            attrs_to_keep = set()  # No attributes to keep for other tags\n",
    "            \n",
    "        # Remove all attributes not in attrs_to_keep\n",
    "        for attr in list(tag.attrs):\n",
    "            if attr not in attrs_to_keep:\n",
    "                del tag[attr]\n",
    "    \n",
    "    # Remove extra whitespace caused by removing tags\n",
    "    cleaned_html = str(soup)\n",
    "    cleaned_html = re.sub(r'\\n{2,}', '\\n', cleaned_html)\n",
    "    cleaned_html = re.sub(r'•\\n', '', cleaned_html)\n",
    "    # Return the cleaned HTML\n",
    "    return cleaned_html\n",
    "\n",
    "# Example usage\n",
    "\n",
    "cleaned_html = fetch_and_clean_html(paper_url)\n",
    "print(cleaned_html)\n",
    "\n",
    "# Save the cleaned HTML to a file\n",
    "with open('cleaned_paper.html', 'w') as file:\n",
    "    file.write(cleaned_html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# GLM: General Language Model Pretraining  \n",
       "with Autoregressive Blank Infilling\n",
       "\n",
       "Zhengxiao Du∗1,2 Yujie Qian∗3 Xiao Liu1,2 Ming Ding1,2 Jiezhong Qiu1,2  \n",
       "Zhilin Yang222Corresponding authors. Jie Tang222Corresponding authors.  \n",
       "1Tsinghua University 2Beijing Academy of Artificial Intelligence (BAAI)  \n",
       "3MIT CSAIL 4Shanghai Qi Zhi Institute  \n",
       "zx-du20@mails.tsinghua.edu.cn yujieq@csail.mit.edu  \n",
       "{zhiliny,jietang}@tsinghua.edu.cn\n",
       "\n",
       "###### Abstract\n",
       "\n",
       "There have been various types of pretraining architectures including\n",
       "autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and\n",
       "encoder-decoder models (e.g., T5). However, none of the pretraining frameworks\n",
       "performs the best for all tasks of three main categories including natural\n",
       "language understanding (NLU), unconditional generation, and conditional\n",
       "generation. We propose a General Language Model (GLM) based on autoregressive\n",
       "blank infilling to address this challenge. GLM improves blank filling\n",
       "pretraining by adding 2D positional encodings and allowing an arbitrary order\n",
       "to predict spans, which results in performance gains over BERT and T5 on NLU\n",
       "tasks. Meanwhile, GLM can be pretrained for different types of tasks by\n",
       "varying the number and lengths of blanks. On a wide range of tasks across NLU,\n",
       "conditional and unconditional generation, GLM outperforms BERT, T5, and GPT\n",
       "given the same model sizes and data, and achieves the best performance from a\n",
       "single pretrained model with 1.25 parameters of BERT, demonstrating its\n",
       "generalizability to different downstream tasks.111The code and pre-trained\n",
       "models are available at <https://github.com/THUDM/GLM>\n",
       "\n",
       "11footnotetext: The first two authors contributed equally.\n",
       "\n",
       "##  1 Introduction\n",
       "\n",
       "Language models pretrained on unlabeled texts have substantially advanced the\n",
       "state of the art in various NLP tasks, ranging from natural language\n",
       "understanding (NLU) to text generation Radford et al. (2018a); Devlin et al.\n",
       "(2019); Yang et al. (2019); Radford et al. (2018b); Raffel et al. (2020);\n",
       "Lewis et al. (2019); Brown et al. (2020). Downstream task performance as well\n",
       "as the scale of the parameters have also constantly increased in the past few\n",
       "years.\n",
       "\n",
       "![](/html/2103.10360/assets/x1.png) Figure 1: Illustration of GLM. We blank\n",
       "out text spans (green part) and generate them autoregressively. (Some\n",
       "attention edges are omitted; cf. Figure 2.)\n",
       "\n",
       "In general, existing pretraining frameworks can be categorized into three\n",
       "families: autoregressive, autoencoding, and encoder-decoder models.\n",
       "Autoregressive models, such as GPT Radford et al. (2018a), learn left-to-right\n",
       "language models. While they succeed in long-text generation and show few-shot\n",
       "learning ability when scaled to billions of parameters Radford et al. (2018b);\n",
       "Brown et al. (2020), the inherent disadvantage is the unidirectional attention\n",
       "mechanism, which cannot fully capture the dependencies between the context\n",
       "words in NLU tasks. Autoencoding models, such as BERT Devlin et al. (2019),\n",
       "learn bidirectional context encoders via denoising objectives, e.g. Masked\n",
       "Language Model (MLM). The encoders produce contextualized representations that\n",
       "suit natural language understanding tasks, but could not be directly applied\n",
       "for text generation. Encoder-decoder models adopt bidirectional attention for\n",
       "the encoder, unidirectional attention for the decoder, and cross attention\n",
       "between them Song et al. (2019); Bi et al. (2020); Lewis et al. (2019). They\n",
       "are typically deployed in conditional generation tasks, such as text\n",
       "summarization and response generation. 222Unconditional generation refers to\n",
       "generating text as a language model without finetuning, while conditional\n",
       "generation refers to sequence-to-sequence tasks.. T5 Raffel et al. (2020)\n",
       "unifies NLU and conditional generation via encoder-decoder models but requires\n",
       "more parameters to match the performance of BRET-based models such as RoBERTa\n",
       "Liu et al. (2019) and DeBERTa He et al. (2021).\n",
       "\n",
       "None of these pretraining frameworks is flexible enough to perform\n",
       "competitively across all NLP tasks. Previous works have tried to unify\n",
       "different frameworks by combining their objectives via multi-task learning\n",
       "Dong et al. (2019); Bao et al. (2020). However, since the autoencoding and\n",
       "autoregressive objectives differ by nature, a simple unification cannot fully\n",
       "inherit the advantages of both frameworks.\n",
       "\n",
       "In this paper, we propose a pretraining framework named GLM (General Language\n",
       "Model), based on autoregressive blank infilling. We randomly blank out\n",
       "continuous spans of tokens from the input text, following the idea of\n",
       "autoencoding, and train the model to sequentially reconstruct the spans,\n",
       "following the idea of autoregressive pretraining (see Figure 1). While\n",
       "blanking filling has been used in T5 Raffel et al. (2020) for text-to-text\n",
       "pretraining, we propose two improvements, namely span shuffling and 2D\n",
       "positional encoding. Empirically, we show that with the same amount of\n",
       "parameters and computational cost, GLM significantly outperforms BERT on the\n",
       "SuperGLUE benchmark by a large margin of 4.6% – 5.0% and outperforms RoBERTa\n",
       "and BART when pretrained on a corpus of similar size (158GB). GLM also\n",
       "significantly outperforms T5 on NLU and generation tasks with fewer parameters\n",
       "and data.\n",
       "\n",
       "Inspired by Pattern-Exploiting Training (PET) Schick and Schütze (2020a), we\n",
       "reformulate NLU tasks as manually-crafted cloze questions that mimic human\n",
       "language. Different from the BERT-based models used by PET, GLM can naturally\n",
       "handle multi-token answers to the cloze question via autoregressive blank\n",
       "filling.\n",
       "\n",
       "Furthermore, we show that by varying the number and lengths of missing spans,\n",
       "the autoregressive blank filling objective can pretrain language models for\n",
       "conditional and unconditional generation. Through multi-task learning of\n",
       "different pretraining objectives, a single GLM can excel in both NLU and\n",
       "(conditional and unconditional) text generation. Empirically, compared with\n",
       "standalone baselines, GLM with multi-task pretraining achieves improvements in\n",
       "NLU, conditional text generation, and language modeling tasks altogether by\n",
       "sharing the parameters.\n",
       "\n",
       "![](/html/2103.10360/assets/x2.png) Figure 2: GLM pretraining. (a) The\n",
       "original text is . Two spans  and  are sampled. (b) Replace the sampled spans\n",
       "with [M] in Part A, and shuffle the spans in Part B. (c) GLM autoregressively\n",
       "generates Part B. Each span is prepended with [S] as input and appended with\n",
       "[E] as output. 2D positional encoding represents inter- and intra-span\n",
       "positions. (d) Self-attention mask. Grey areas are masked out. Part A tokens\n",
       "can attend to themselves (blue frame) but not B. Part B tokens can attend to A\n",
       "and their antecedents in B (yellow and green frames correspond to the two\n",
       "spans). , , and .\n",
       "\n",
       "##  2 GLM Pretraining Framework\n",
       "\n",
       "We propose a general pretraining framework GLM based on a novel autoregressive\n",
       "blank infilling objective. GLM formulates NLU tasks as cloze questions that\n",
       "contain task descriptions, which can be answered by autoregressive generation.\n",
       "\n",
       "###  2.1 Pretraining Objective\n",
       "\n",
       "####  2.1.1 Autoregressive Blank Infilling\n",
       "\n",
       "GLM is trained by optimizing an autoregressive blank infilling objective.\n",
       "Given an input text , multiple text spans  are sampled, where each span\n",
       "corresponds to a series of consecutive tokens  in . Each span is replaced with\n",
       "a single  token, forming a corrupted text . The model predicts the missing\n",
       "tokens in the spans from the corrupted text in an autoregressive manner, which\n",
       "means when predicting the missing tokens in a span, the model has access to\n",
       "the corrupted text _and_ the previously predicted spans. To fully capture the\n",
       "interdependencies between different spans, we randomly permute the order of\n",
       "the spans, similar to the permutation language model Yang et al. (2019).\n",
       "Formally, let  be the set of all possible permutations of the length- index\n",
       "sequence , and  be , we define the pretraining objective as\n",
       "\n",
       "|  |  | (1)  \n",
       "---|---|---|---  \n",
       "  \n",
       "We always generate the tokens in each blank following a left-to-right order,\n",
       "i.e. the probability of generating the span  is factorized as:\n",
       "\n",
       "|  |  | (2)  \n",
       "---|---|---|---  \n",
       "  \n",
       "We implement the autoregressive blank infilling objective with the following\n",
       "techniques. The input  is divided into two parts: Part A is the corrupted text\n",
       ", and Part B consists of the masked spans. Part A tokens can attend to each\n",
       "other, but cannot attend to any tokens in B. Part B tokens can attend to Part\n",
       "A and antecedents in B, but cannot attend to any subsequent tokens in B. To\n",
       "enable autoregressive generation, each span is padded with special tokens  and\n",
       ", for input and output respectively. In this way, our model automatically\n",
       "learns a bidirectional encoder (for Part A) and a unidirectional decoder (for\n",
       "Part B) in a unified model. The implementation of GLM is illustrated in Figure\n",
       "2.\n",
       "\n",
       "We randomly sample spans of length drawn from a Poisson distribution with . We\n",
       "repeatedly sample new spans until at least 15% of the original tokens are\n",
       "masked. Empirically, we have found that the 15% ratio is critical for good\n",
       "performance on downstream NLU tasks.\n",
       "\n",
       "####  2.1.2 Multi-Task Pretraining\n",
       "\n",
       "In the previous section, GLM masks short spans and is suited for NLU tasks.\n",
       "However, we are interested in pretraining a single model that can handle both\n",
       "NLU and text generation. We then study a multi-task pretraining setup, in\n",
       "which a second objective of generating longer text is jointly optimized with\n",
       "the blank infilling objective. We consider the following two objectives:\n",
       "\n",
       "  * Document-level. We sample a single span whose length is sampled from a uniform distribution over 50%–100% of the original length. The objective aims for long text generation.\n",
       "\n",
       "  * Sentence-level. We restrict that the masked spans must be full sentences. Multiple spans (sentences) are sampled to cover 15% of the original tokens. This objective aims for seq2seq tasks whose predictions are often complete sentences or paragraphs.\n",
       "\n",
       "Both new objectives are defined in the same way as the original objective,\n",
       "i.e. Eq. 1. The only difference is the number of spans and the span lengths.\n",
       "\n",
       "###  2.2 Model Architecture\n",
       "\n",
       "GLM uses a single Transformer with several modifications to the architecture:\n",
       "(1) we rearrange the order of layer normalization and the residual connection,\n",
       "which has been shown critical for large-scale language models to avoid\n",
       "numerical errors Shoeybi et al. (2019); (2) we use a single linear layer for\n",
       "the output token prediction; (3) we replace ReLU activation functions with\n",
       "GeLUs Hendrycks and Gimpel (2016).\n",
       "\n",
       "####  2.2.1 2D Positional Encoding\n",
       "\n",
       "One of the challenges of the autoregressive blank infilling task is how to\n",
       "encode the positional information. Transformers rely on positional encodings\n",
       "to inject the absolute and relative positions of the tokens. We propose 2D\n",
       "positional encodings to address the challenge. Specifically, each token is\n",
       "encoded with two positional ids. The first positional id represents the\n",
       "position in the corrupted text . For the masked spans, it is the position of\n",
       "the corresponding  token. The second positional id represents the intra-span\n",
       "position. For tokens in Part A, their second positional ids are . For tokens\n",
       "in Part B, they range from 1 to the length of the span. The two positional ids\n",
       "are projected into two vectors via learnable embedding tables, which are both\n",
       "added to the input token embeddings.\n",
       "\n",
       "Our encoding method ensures that the model is not aware of the length of the\n",
       "masked span when reconstructing them. It is an important difference as\n",
       "compared to other models. For example, XLNet Yang et al. (2019) encodes the\n",
       "original position so that it can perceive the number of missing tokens, and\n",
       "SpanBERT Joshi et al. (2020) replaces the span with multiple [MASK] tokens and\n",
       "keeps the length unchanged. Our design fits downstream tasks as usually the\n",
       "length of the generated text is unknown beforehand.\n",
       "\n",
       "![](/html/2103.10360/assets/x3.png) Figure 3: Formulation of the sentiment\n",
       "classification task as blank infilling with GLM.\n",
       "\n",
       "###  2.3 Finetuning GLM\n",
       "\n",
       "Typically, for downstream NLU tasks, a linear classifier takes the\n",
       "representations of sequences or tokens produced by pretrained models as input\n",
       "and predicts the correct labels. The practices are different from the\n",
       "generative pretraining task, leading to inconsistency between pretraining and\n",
       "finetuning.\n",
       "\n",
       "Instead, we reformulate NLU classification tasks as generation tasks of blank\n",
       "infilling, following PET Schick and Schütze (2020a). Specifically, given a\n",
       "labeled example , we convert the input text  to a cloze question  via a\n",
       "pattern containing a single mask token. The pattern is written in natural\n",
       "language to represent the semantics of the task. For example, a sentiment\n",
       "classification task can be formulated as “{SENTENCE}. It’s really ”. The\n",
       "candidate labels  are also mapped to answers to the cloze, called verbalizer .\n",
       "In sentiment classification, the labels “positive” and “negative” are mapped\n",
       "to the words “good” and “bad”. The conditional probability of predicting\n",
       "given  is\n",
       "\n",
       "|  |  | (3)  \n",
       "---|---|---|---  \n",
       "  \n",
       "where  is the label set. Therefore the probability of the sentence being\n",
       "positive or negative is proportional to predicting “good” or “bad” in the\n",
       "blank. Then we finetune GLM with a cross-entropy loss (see Figure 3).\n",
       "\n",
       "For text generation tasks, the given context constitutes the Part A of the\n",
       "input, with a mask token appended at the end. The model generates the text of\n",
       "Part B autoregressively. We can directly apply the pretrained GLM for\n",
       "unconditional generation, or finetune it on downstream conditional generation\n",
       "tasks.\n",
       "\n",
       "###  2.4 Discussion and Analysis\n",
       "\n",
       "In this section, we discuss the differences between GLM and other pretraining\n",
       "models. We are mainly concerned with how they can be adapted to downstream\n",
       "blank infilling tasks.\n",
       "\n",
       "Comparison with BERT Devlin et al. (2019). As pointed out by Yang et al.\n",
       "(2019), BERT fails to capture the interdependencies of masked tokens due to\n",
       "the independence assumption of MLM. Another disadvantage of BERT is that it\n",
       "cannot fill in the blanks of multiple tokens properly. To infer the\n",
       "probability of an answer of length , BERT needs to perform  consecutive\n",
       "predictions. If the length  is unknown, we may need to enumerate all possible\n",
       "lengths, since BERT needs to change the number of  tokens according to the\n",
       "length.\n",
       "\n",
       "Comparison with XLNet Yang et al. (2019). Both GLM and XLNet are pretrained\n",
       "with autoregressive objectives, but there are two differences between them.\n",
       "First, XLNet uses the original position encodings before corruption. During\n",
       "inference, we need to either know or enumerate the length of the answer, the\n",
       "same problem as BERT. Second, XLNet uses a two-stream self-attention\n",
       "mechanism, instead of the right-shift, to avoid the information leak within\n",
       "Transformer. It doubles the time cost of pretraining.\n",
       "\n",
       "Comparison with T5 Raffel et al. (2020). T5 proposes a similar blank infilling\n",
       "objective to pretrain an encoder-decoder Transformer. T5 uses independent\n",
       "positional encodings for the encoder and decoder, and relies on multiple\n",
       "sentinel tokens to differentiate the masked spans. In downstream tasks, only\n",
       "one of the sentinel tokens is used, leading to a waste of model capacity and\n",
       "inconsistency between pretraining and finetuning. Moreover, T5 always predicts\n",
       "spans in a fixed left-to-right order. As a result, GLM can significantly\n",
       "outperform T5 on NLU and seq2seq tasks with fewer parameters and data, as\n",
       "stated in Sections 3.2 and 3.3.\n",
       "\n",
       "Comparison with UniLM Dong et al. (2019). UniLM combines different pretraining\n",
       "objectives under the autoencoding framework by changing the attention mask\n",
       "among bidirectional, unidirectional, and cross attention. However, UniLM\n",
       "always replaces masked spans with [MASK] tokens, which limits its ability to\n",
       "model the dependencies between the masked spans and their context. GLM feeds\n",
       "in the previous token and autoregressively generates the next token.\n",
       "Finetuning UniLM on downstream generation tasks also relies on masked language\n",
       "modeling, which is less efficient. UniLMv2 Bao et al. (2020) adopts partially\n",
       "autoregressive modeling for generation tasks, along with the autoencoding\n",
       "objective for NLU tasks. Instead, GLM unifies NLU and generation tasks with\n",
       "autoregressive pretraining.\n",
       "\n",
       "Table 1: Results on the SuperGLUE dev set. | Model |  ReCoRD F1/Acc.  |  COPA Acc.  |  WSC Acc.  |  RTE Acc.  |  BoolQ Acc.  |  WiC Acc.  |  CB F1/Acc.  |  MultiRC F1a/EM  | Avg  \n",
       "---|---|---|---|---|---|---|---|---|---|---  \n",
       "Pretrained on BookCorpus and Wikipedia |  |  |  |  |  |   \n",
       "| BERT | 65.4 / 64.9 | 66.0 | 65.4 | 70.0 | 74.9 | 68.8 | 70.9 / 76.8 | 68.4 / 21.5 | 66.1  \n",
       "| GLM | 73.5 / 72.8 | 71.0 | 72.1 | 71.2 | 77.0 | 64.7 | 89.5 / 85.7 | 72.1 / 26.1 | 70.7  \n",
       "| BERT | 76.3 / 75.6 | 69.0 | 64.4 | 73.6 | 80.1 | 71.0 | 94.8 / 92.9 | 71.9 / 24.1 | 72.0  \n",
       "| UniLM | 80.0 / 79.1 | 72.0 | 65.4 | 76.5 | 80.5 | 69.7 | 91.0 / 91.1 | 77.2 / 38.2 | 74.1  \n",
       "| GLM | 81.7 / 81.1 | 76.0 | 81.7 | 74.0 | 82.1 | 68.5 | 96.1 / 94.6 | 77.1 / 36.3 | 77.0  \n",
       "| GLM | 80.2 / 79.6 | 77.0 | 78.8 | 76.2 | 79.8 | 63.6 | 97.3 / 96.4 | 74.6 / 32.1 | 75.7  \n",
       "| GLM | 80.7 / 80.2 | 77.0 | 79.8 | 79.1 | 80.8 | 70.4 | 94.6 / 93.7 | 76.9 / 36.1 | 76.8  \n",
       "| GLM | 81.5 / 80.9 | 80.0 | 81.7 | 79.4 | 81.9 | 69.0 | 93.2 / 96.4  | 76.2 / 35.5 | 78.0  \n",
       "| GLM | 82.3 / 81.7 | 85.0 | 81.7 | 79.1 | 81.3 | 69.4 | 95.0 / 96.4  |  77.2 / 35.0 | 78.8  \n",
       "Pretrained on larger corpora |  |  |  |  |  |   \n",
       "| T5 | 76.2 / 75.4 | 73.0 | 79.8 | 78.3 | 80.8 | 67.9 | 94.8 / 92.9 | 76.4 / 40.0 | 76.0  \n",
       "| T5 | 85.7 / 85.0 | 78.0 | 84.6 | 84.8 | 84.3 | 71.6 | 96.4 / 98.2 | 80.9 / 46.6 | 81.2  \n",
       "| BART | 88.3 / 87.8 | 60.0 | 65.4 | 84.5 | 84.3 | 69.0 | 90.5 / 92.9 | 81.8 / 48.0 | 76.0  \n",
       "| RoBERTa | 89.0 / 88.4 | 90.0 | 63.5 | 87.0 | 86.1 | 72.6 | 96.1 / 94.6 | 84.4 / 52.9 | 81.5  \n",
       "| GLM | 89.6 / 89.0 | 82.0 | 83.7 | 87.7 | 84.7 | 71.2 | 98.7 / 98.2 | 82.4 / 50.1 | 82.9  \n",
       "  \n",
       "##  3 Experiments\n",
       "\n",
       "We now describe our pretraining setup and the evaluation of downstream tasks.\n",
       "\n",
       "###  3.1 Pretraining Setup\n",
       "\n",
       "For a fair comparison with BERT Devlin et al. (2019), we use BooksCorpus Zhu\n",
       "et al. (2015) and English Wikipedia as our pretraining data. We use the\n",
       "uncased wordpiece tokenizer of BERT with 30k vocabulary. We train GLM and GLM\n",
       "with the same architectures as BERT and BERT, containing 110M and 340M\n",
       "parameters respectively.\n",
       "\n",
       "For multi-task pretraining, we train two Large-sized models with a mixture of\n",
       "the blank infilling objective and the document-level or sentence-level\n",
       "objective, denoted as GLM and GLM. Additionally, we train two larger GLM\n",
       "models of 410M (30 layers, hidden size 1024, and 16 attention heads) and 515M\n",
       "(30 layers, hidden size 1152, and 18 attention heads) parameters with\n",
       "document-level multi-task pretraining, denoted as GLM and GLM.\n",
       "\n",
       "To compare with SOTA models, we also train a Large-sized model with the same\n",
       "data, tokenization, and hyperparameters as RoBERTa Liu et al. (2019), denoted\n",
       "as GLM. Due to resource limitations, we only pretrain the model for 250,000\n",
       "steps, which are half of RoBERTa and BART’s training steps and close to T5 in\n",
       "the number of trained tokens. More experiment details can be found in Appendix\n",
       "A.\n",
       "\n",
       "Table 2: Results of abstractive summarization on the CNN/DailyMail and XSum test sets. Model  | CNN/DailyMail | XSum  \n",
       "---|---|---  \n",
       "RG-1 | RG-2 | RG-L | RG-1 | RG-2 | RG-L  \n",
       "BERTSumAbs Liu and Lapata (2019) | 41.7 | 19.4 | 38.8 | 38.8 | 16.3 | 31.2  \n",
       "UniLMv2 Bao et al. (2020) | 43.2 | 20.4 | 40.1 | 44.0 | 21.1 | 36.1  \n",
       "T5 Raffel et al. (2020) | 42.5 | 20.7 | 39.8 | 40.9 | 17.3 | 33.0  \n",
       "BART Lewis et al. (2019) | 44.2 | 21.3 | 40.9 | 45.1 | 22.3 | 37.3  \n",
       "GLM | 43.8 | 21.0 | 40.5 | 45.5 | 23.5 | 37.3  \n",
       "  \n",
       "###  3.2 SuperGLUE\n",
       "\n",
       "To evaluate our pretrained GLM models, we conduct experiments on the SuperGLUE\n",
       "benchmark Wang et al. (2019) and report the standard metrics. SuperGLUE\n",
       "consists of 8 challenging NLU tasks. We reformulate the classification tasks\n",
       "as blank infilling with human-crafted cloze questions, following PET Schick\n",
       "and Schütze (2020b). Then we finetune the pretrained GLM models on each task\n",
       "as described in Section 2.3. The cloze questions and other details can be\n",
       "found in Section B.1.\n",
       "\n",
       "For a fair comparison with GLM and GLM, we choose BERT and BERT as our\n",
       "baselines, which are pretrained on the same corpus and for a similar amount of\n",
       "time. We report the performance of standard finetuning (i.e. classification on\n",
       "the [CLS] token representation). The performance of BERT with cloze questions\n",
       "is reported in Section 3.4. To compare with GLM, we choose T5, BART, and\n",
       "RoBERTa as our baselines. T5 has no direct match in the number of parameters\n",
       "for BERT, so we present the results of both T5 (220M parameters) and T5 (770M\n",
       "parameters). All the other baselines are of similar size to BERT.\n",
       "\n",
       "Table 1 shows the results. With the same amount of training data, GLM\n",
       "consistently outperforms BERT on most tasks with either base or large\n",
       "architecture. The only exception is WiC (word sense disambiguation). On\n",
       "average, GLM scores 4.6% higher than BERT, and GLM scores 5.0% higher than\n",
       "BERT. It clearly demonstrates the advantage of our method in NLU tasks. In the\n",
       "setting of RoBERTa, GLM can still achieve improvements over the baselines, but\n",
       "with a smaller margin. Specifically, GLM outperforms T5 but is only half its\n",
       "size. We also find that BART does not perform well on the challenging\n",
       "SuperGLUE benchmark. We conjecture this can be attributed to the low parameter\n",
       "efficiency of the encoder-decoder architecture and the denoising sequence-to-\n",
       "sequence objective.\n",
       "\n",
       "###  3.3 Multi-Task Pretraining\n",
       "\n",
       "Then we evaluate the GLM’s performance in a multi-task setting (Section 2.1).\n",
       "Within one training batch, we sample short spans and longer spans (document-\n",
       "level or sentence-level) with equal chances. We evaluate the multi-task model\n",
       "for NLU, seq2seq, blank infilling, and zero-shot language modeling.\n",
       "\n",
       "SuperGLUE. For NLU tasks, we evaluate models on the SuperGLUE benchmark. The\n",
       "results are also shown in Table 1. We observe that with multi-task\n",
       "pretraining, GLM and GLM perform slightly worse than GLM, but still outperform\n",
       "BERT and UniLM. Among multi-task models, GLM outperforms GLM by 1.1% on\n",
       "average. Increasing GLM’s parameters to 410M (1.25BERT) leads to better\n",
       "performance than GLM. GLM with 515M parameters (1.5BERT) can perform even\n",
       "better.\n",
       "\n",
       "Table 3: Results on Gigaword summarization. Model | RG-1 | RG-2 | RG-L  \n",
       "---|---|---|---  \n",
       "MASS | 37.7 | 18.5 | 34.9  \n",
       "UniLM | 38.5 | 19.5 | 35.8  \n",
       "GLM | 38.6 | 19.7 | 36.0  \n",
       "GLM | 38.5 | 19.4 | 35.8  \n",
       "GLM | 38.9 | 20.0 | 36.3  \n",
       "GLM | 38.9 | 20.0 | 36.2  \n",
       "Table 4: Results on SQuAD question generation. Model | BLEU-4 | MTR | RG-L  \n",
       "---|---|---|---  \n",
       "SemQG | 18.4 | 22.7 | 46.7  \n",
       "UniLM | 22.1 | 25.1 | 51.1  \n",
       "GLM | 22.4 | 25.2 | 50.4  \n",
       "GLM | 22.3 | 25.0 | 50.2  \n",
       "GLM | 22.6 | 25.4 | 50.4  \n",
       "GLM | 22.9 | 25.6 | 50.5  \n",
       "Table 5: BLEU scores on Yahoo text infilling. † indicates the results from Shen et al. (2020). Mask ratio | 10% | 20% | 30% | 40% | 50%  \n",
       "---|---|---|---|---|---  \n",
       "BERT† | 82.8 | 66.3 | 50.3 | 37.4 | 26.2  \n",
       "BLM† | 86.5 | 73.2 | 59.6 | 46.8 | 34.8  \n",
       "GLM | 87.8 | 76.7 | 64.2 | 48.9 | 38.7  \n",
       "GLM | 87.5 | 76.0 | 63.2 | 47.9 | 37.6  \n",
       "  \n",
       "Sequence-to-Sequence. Considering the available baseline results, we use the\n",
       "Gigaword dataset Rush et al. (2015) for abstractive summarization and the\n",
       "SQuAD 1.1 dataset Rajpurkar et al. (2016) for question generation Du et al.\n",
       "(2017) as the benchmarks for models pretrained on BookCorpus and Wikipedia.\n",
       "Additionally, we use the CNN/DailyMail See et al. (2017) and XSum Narayan et\n",
       "al. (2018) datasets for abstractive summarization as the benchmarks for models\n",
       "pretrained on larger corpora.\n",
       "\n",
       "The results for models trained on BookCorpus and Wikipedia are shown in Tables\n",
       "5 and 5. We observe that GLM can achieve performance matching the other\n",
       "pretraining models on the two generation tasks. GLM can perform better than\n",
       "GLM, while GLM performs slightly worse than GLM. This indicates that the\n",
       "document-level objective, which teaches the model to extend the given\n",
       "contexts, is less helpful to conditional generation, which aims to extract\n",
       "useful information from the context. Increasing GLM’s parameters to 410M leads\n",
       "to the best performance on both tasks. The results for models trained on\n",
       "larger corpora are shown in Table 2. GLM can achieve performance matching the\n",
       "seq2seq BART model, and outperform T5 and UniLMv2.\n",
       "\n",
       "Text Infilling. Text infilling is the task of predicting missing spans of text\n",
       "which are consistent with the surrounding context Zhu et al. (2019); Donahue\n",
       "et al. (2020); Shen et al. (2020). GLM is trained with an autoregressive blank\n",
       "infilling objective, thus can straightforwardly solve this task. We evaluate\n",
       "GLM on the Yahoo Answers dataset Yang et al. (2017) and compare it with Blank\n",
       "Language Model (BLM) Shen et al. (2020), which is a specifically designed\n",
       "model for text infilling. From the results in Table 5, GLM outperforms\n",
       "previous methods by large margins (1.3 to 3.9 BLEU) and achieves the state-of-\n",
       "the-art result on this dataset. We notice that GLM slightly underperforms GLM,\n",
       "which is consistent with our observations in the seq2seq experiments.\n",
       "\n",
       "Language Modeling. Most language modeling datasets such as WikiText103 are\n",
       "constructed from Wikipedia documents, which our pretraining dataset already\n",
       "contains. Therefore, we evaluate the language modeling perplexity on a held-\n",
       "out test set of our pretraining dataset, which contains about 20M tokens,\n",
       "denoted as BookWiki. We also evaluate GLM on the LAMBADA dataset Paperno et\n",
       "al. (2016), which tests the ability of systems to model long-range\n",
       "dependencies in text. The task is to predict the final word of a passage. As\n",
       "the baseline, we train a GPT model Radford et al. (2018b); Brown et al. (2020)\n",
       "with the same data and tokenization as GLM.\n",
       "\n",
       "The results are shown in Figure 4. All the models are evaluated in the zero-\n",
       "shot setting. Since GLM learns the bidirectional attention, we also evaluate\n",
       "GLM under the setting in which the contexts are encoded with bidirectional\n",
       "attention. Without generative objective during pretraining, GLM cannot\n",
       "complete the language modeling tasks, with perplexity larger than 100. With\n",
       "the same amount of parameters, GLM performs worse than GPT. This is expected\n",
       "since GLM also optimizes the blank infilling objective. Increasing the model’s\n",
       "parameters to 410M (1.25 of GPT) leads to a performance close to GPT. GLM (1.5\n",
       "of GPT) can further outperform GPT. With the same amount of parameters,\n",
       "encoding the context with bidirectional attention can improve the performance\n",
       "of language modeling. Under this setting, GLM outperforms GPT. This is the\n",
       "advantage of GLM over unidirectional GPT. We also study the contribution of 2D\n",
       "positional encoding to long text generation. We find that removing the 2D\n",
       "positional encoding leads to lower accuracy and higher perplexity in language\n",
       "modeling.\n",
       "\n",
       "![](/html/2103.10360/assets/x4.png) Figure 4: Zero-shot language modeling results. Table 6: Ablation study on the SuperGLUE dev set. (T5  GLM – shuffle spans + sentinel tokens.) Model |  ReCoRD F1/Acc.  |  COPA Acc.  |  WSC Acc.  |  RTE Acc.  |  BoolQ Acc.  |  WiC Acc.  |  CB F1/Acc.  |  MultiRC F1a/EM  | Avg  \n",
       "---|---|---|---|---|---|---|---|---|---  \n",
       "BERT | 76.3 / 75.6 | 69.0 | 64.4 | 73.6 | 80.1 | 71.0 | 94.8 / 92.9 | 71.9 / 24.1 | 72.0  \n",
       "BERT (reproduced) | 82.1 / 81.5 | 63.0 | 63.5 | 72.2 | 80.8 | 68.7 | 80.9 / 85.7 | 77.0 / 35.2 | 71.2  \n",
       "BERT (cloze) | 70.0 / 69.4 | 80.0 | 76.0 | 72.6 | 78.1 | 70.5 | 93.5 / 91.1 | 70.0 / 23.1 | 73.2  \n",
       "GLM | 81.7 / 81.1 | 76.0 | 81.7 | 74.0 | 82.1 | 68.5 | 96.1 / 94.6 | 77.1 / 36.3 | 77.0  \n",
       "– cloze finetune | 81.3 / 80.6 | 62.0 | 63.5 | 66.8 | 80.5 | 65.0 | 89.2 / 91.1 | 72.3 / 27.9 | 70.0  \n",
       "– shuffle spans | 82.0 / 81.4 | 61.0 | 79.8 | 54.5 | 65.8 | 56.3 | 90.5 / 92.9 | 76.7 / 37.6 | 68.5  \n",
       "\\+ sentinel tokens | 81.8 / 81.3 | 69.0 | 78.8 | 77.3 | 81.2 | 68.0 | 93.7 / 94.6 | 77.5 / 37.7 | 76.0  \n",
       "  \n",
       "Summary. Above all, we conclude that GLM effectively shares model parameters\n",
       "across natural language understanding and generation tasks, achieving better\n",
       "performance than a standalone BERT, encoder-decoder, or GPT model.\n",
       "\n",
       "###  3.4 Ablation Study\n",
       "\n",
       "Table 6 shows our ablation analysis for GLM. First, to provide an apple-to-\n",
       "apple comparison with BERT, we train a BERT model with our implementation,\n",
       "data, and hyperparameters (row 2). The performance is slightly worse than the\n",
       "official BERT and significantly worse than GLM. It confirms the superiority of\n",
       "GLM over Masked LM pretraining on NLU tasks. Second, we show the SuperGLUE\n",
       "performance of GLM finetuned as sequence classifiers (row 5) and BERT with\n",
       "cloze-style finetuning (row 3). Compared to BERT with cloze-style finetuning,\n",
       "GLM benefits from the autoregressive pretraining. Especially on ReCoRD and\n",
       "WSC, where the verbalizer consists of multiple tokens, GLM consistently\n",
       "outperforms BERT. This demonstrates GLM’s advantage in handling variable-\n",
       "length blank. Another observation is that the cloze formulation is critical\n",
       "for GLM’s performance on NLU tasks. For the large model, cloze-style\n",
       "finetuning can improve the performance by 7 points. Finally, we compare GLM\n",
       "variants with different pretraining designs to understand their importance.\n",
       "Row 6 shows that removing the span shuffling (always predicting the masked\n",
       "spans from left to right) leads to a severe performance drop on SuperGLUE. Row\n",
       "7 uses different sentinel tokens instead of a single  token to represent\n",
       "different masked spans. The model performs worse than the standard GLM. We\n",
       "hypothesize that it wastes some modeling capacity to learn the different\n",
       "sentinel tokens which are not used in downstream tasks with only one blank. In\n",
       "Figure 4, we show that removing the second dimension of 2D positional encoding\n",
       "hurts the performance of long text generation.\n",
       "\n",
       "We note that T5 is pretrained with a similar blank infilling objective. GLM\n",
       "differs in three aspects: (1) GLM consists of a single encoder, (2) GLM\n",
       "shuffles the masked spans, and (3) GLM uses a single [MASK] instead of\n",
       "multiple sentinel tokens. While we cannot directly compare GLM with T5 due to\n",
       "the differences in training data and the number of parameters, the results in\n",
       "Tables 1 and 6 have demonstrated the advantage of GLM.\n",
       "\n",
       "##  4 Related Work\n",
       "\n",
       "Pretrained Language Models. Pretraining large-scale language models\n",
       "significantly improves the performance of downstream tasks. There are three\n",
       "types of pretrained models. First, autoencoding models learn a bidirectional\n",
       "contextualized encoder for natural language understanding via denoising\n",
       "objectives Devlin et al. (2019); Joshi et al. (2020); Yang et al. (2019); Liu\n",
       "et al. (2019); Lan et al. (2020); Clark et al. (2020). Second, autoregressive\n",
       "models are trained with a left-to-right language modeling objective Radford et\n",
       "al. (2018a, b); Brown et al. (2020). Third, encoder-decoder models are\n",
       "pretrained for sequence-to-sequence tasks Song et al. (2019); Lewis et al.\n",
       "(2019); Bi et al. (2020); Zhang et al. (2020).\n",
       "\n",
       "Among encoder-decoder models, BART Lewis et al. (2019) conducts NLU tasks by\n",
       "feeding the same input into the encoder and decoder, and taking the final\n",
       "hidden states of the decoder. Instead, T5 Raffel et al. (2020) formulates most\n",
       "language tasks in the text-to-text framework. However, both models require\n",
       "more parameters to outperform autoencoding models such as RoBERTa Liu et al.\n",
       "(2019). UniLM Dong et al. (2019); Bao et al. (2020) unifies three pretraining\n",
       "models under the masked language modeling objective with different attention\n",
       "masks.\n",
       "\n",
       "NLU as Generation. Previously, pretrained language models complete\n",
       "classification tasks for NLU with linear classifiers on the learned\n",
       "representations. GPT-2 Radford et al. (2018b) and GPT-3 Brown et al. (2020)\n",
       "show that generative language models can complete NLU tasks such as question\n",
       "answering by directly predicting the correct answers without finetuning, given\n",
       "task instructions or a few labeled examples. However, generative models\n",
       "require much more parameters to work due to the limit of unidirectional\n",
       "attention. Recently, PET Schick and Schütze (2020a, b) proposes to reformulate\n",
       "input examples as cloze questions with patterns similar to the pretraining\n",
       "corpus in the few-shot setting. It has been shown that combined with gradient-\n",
       "based finetuning, PET can achieve better performance in the few-shot setting\n",
       "than GPT-3 while requiring only 0.1% of its parameters. Similarly,\n",
       "Athiwaratkun et al. (2020) and Paolini et al. (2020) convert structured\n",
       "prediction tasks, such as sequence tagging and relation extraction, to\n",
       "sequence generation tasks.\n",
       "\n",
       "Blank Language Modeling. Donahue et al. (2020) and Shen et al. (2020) also\n",
       "study blanking infilling models. Different from their work, we pre-train\n",
       "language models with blank infilling objectives and evaluate their performance\n",
       "in downstream NLU and generation tasks.\n",
       "\n",
       "##  5 Conclusions\n",
       "\n",
       "GLM is a general pretraining framework for natural language understanding and\n",
       "generation. We show that the NLU tasks can be formulated as conditional\n",
       "generation tasks, and therefore solvable by autoregressive models. GLM unifies\n",
       "the pretraining objectives for different tasks as autoregressive blank\n",
       "infilling, with mixed attention masks and the novel 2D position encodings.\n",
       "Empirically we show that GLM outperforms previous methods for NLU tasks and\n",
       "can effectively share parameters for different tasks.\n",
       "\n",
       "## Acknowledgements\n",
       "\n",
       "The work is supported by the NSFC for Distinguished Young Scholar(61825602),\n",
       "and Beijing Academy of Artificial Intelligence (BAAI).\n",
       "\n",
       "## References\n",
       "\n",
       "  * Athiwaratkun et al. (2020) Ben Athiwaratkun, Cicero dos Santos, Jason Krone, and Bing Xiang. 2020. [Augmented natural language for generative sequence labeling](https://doi.org/10.18653/v1/2020.emnlp-main.27). In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_ , pages 375–385. \n",
       "  * Bao et al. (2020) Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang, Jianfeng Gao, Songhao Piao, Ming Zhou, and Hsiao-Wuen Hon. 2020. [Unilmv2: Pseudo-masked language models for unified language model pre-training](http://arxiv.org/abs/2002.12804). In _ICML 2020_ , volume 119, pages 642–652. \n",
       "  * Bi et al. (2020) Bin Bi, Chenliang Li, Chen Wu, Ming Yan, Wei Wang, Songfang Huang, Fei Huang, and Luo Si. 2020. [PALM: Pre-training an Autoencoding&Autoregressive Language Model for Context-conditioned Generation](https://doi.org/10.18653/v1/2020.emnlp-main.700). In _EMNLP 2020_ , pages 8681–8691. \n",
       "  * Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. [Language Models are Few-Shot Learners](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html). In _NeurIPS 2020_. \n",
       "  * Cer et al. (2017) Daniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-Gazpio, and Lucia Specia. 2017. [SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation](https://aclanthology.org/S17-2001). In _Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)_ , pages 1–14. \n",
       "  * Clark et al. (2020) Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. 2020. [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://openreview.net/forum?id=r1xMH1BtvB). In _ICLR 2020_. \n",
       "  * Dagan et al. (2005) Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. [The pascal recognising textual entailment challenge](https://doi.org/10.1007/11736790_9). In _Machine Learning Challenges Workshop_ , pages 177–190. Springer. \n",
       "  * Denkowski and Lavie (2014) Michael Denkowski and Alon Lavie. 2014. [Meteor Universal: Language Specific Translation Evaluation for Any Target Language](https://www.aclweb.org/anthology/W14-3348). In _Proceedings of the Ninth Workshop on Statistical Machine Translation_ , pages 376–380. \n",
       "  * Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://aclanthology.org/N19-1423). In _NAACL 2019_ , pages 4171–4186. \n",
       "  * Donahue et al. (2020) Chris Donahue, Mina Lee, and Percy Liang. 2020. [Enabling language models to fill in the blanks](https://doi.org/10.18653/v1/2020.acl-main.225). pages 2492–2501. \n",
       "  * Dong et al. (2019) Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2019. [Unified language model pre-training for natural language understanding and generation](https://proceedings.neurips.cc/paper/2019/file/c20bb2d9a50d5ac1f713f8b34d9aac5a-Paper.pdf). In _NeurIPS 2019_ , pages 13042–13054. \n",
       "  * Du et al. (2017) Xinya Du, Junru Shao, and Claire Cardie. 2017. [Learning to Ask: Neural Question Generation for Reading Comprehension](https://doi.org/10.18653/v1/P17-1123). In _ACL 2017_ , pages 1342–1352. \n",
       "  * Gokaslan and Cohen (2019) Aaron Gokaslan and Vanya Cohen. 2019. Openwebtext corpus. <http://Skylion007.github.io/OpenWebTextCorpus>. \n",
       "  * He et al. (2021) Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2021. [Deberta: Decoding-enhanced bert with disentangled attention](http://arxiv.org/abs/2006.03654). _ArXiv_ , abs/2006.03654. \n",
       "  * Hendrycks and Gimpel (2016) Dan Hendrycks and Kevin Gimpel. 2016. [Bridging nonlinearities and stochastic regularizers with gaussian error linear units](http://arxiv.org/abs/1606.08415). _CoRR_ , abs/1606.08415. \n",
       "  * Joshi et al. (2020) Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. 2020. [SpanBERT: Improving Pre-training by Representing and Predicting Spans](https://transacl.org/ojs/index.php/tacl/article/view/1853). _Trans. Assoc. Comput. Linguistics_ , 8:64–77. \n",
       "  * Lan et al. (2020) Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020. [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://openreview.net/forum?id=H1eA7AEtvS). In _ICLR 2020_. \n",
       "  * Lewis et al. (2019) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://doi.org/10.18653/v1/2020.acl-main.703). In _ACL 2020_ , pages 7871–7880. \n",
       "  * Lin (2004) Chin-Yew Lin. 2004. [ROUGE: A Package for Automatic Evaluation of Summaries](https://www.aclweb.org/anthology/W04-1013). pages 74–81. \n",
       "  * Liu and Lapata (2019) Yang Liu and Mirella Lapata. 2019. [Text Summarization with Pretrained Encoders](https://www.aclweb.org/anthology/D19-1387). In _EMNLP 2019_ , pages 3730–3740. \n",
       "  * Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. [Roberta: A robustly optimized BERT pretraining approach](http://arxiv.org/abs/1907.11692). _CoRR_ , abs/1907.11692. \n",
       "  * Mackenzie et al. (2020) Joel Mackenzie, Rodger Benham, Matthias Petri, Johanne R. Trippas, J. Shane Culpepper, and Alistair Moffat. 2020. [CC-News-En: A Large English News Corpus](https://dl.acm.org/doi/10.1145/3340531.3412762). In _CIKM 2020_ , pages 3077–3084. \n",
       "  * Narayan et al. (2018) Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. [Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization](https://aclanthology.org/D18-1206). In _EMNLP 2018_ , pages 1797–1807. \n",
       "  * Paolini et al. (2020) Giovanni Paolini, Ben Athiwaratkun, Jason Krone, Jie Ma, Alessandro Achille, Rishita Anubhai, Cicero Nogueira dos Santos, Bing Xiang, and Stefano Soatto. 2020\\. [Structured Prediction as Translation between Augmented Natural Languages](https://openreview.net/forum?id=US-TP-xnXI). \n",
       "  * Paperno et al. (2016) Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. 2016. [The LAMBADA dataset: Word prediction requiring a broad discourse context](https://doi.org/10.18653/v1/p16-1144). In _ACL 2016_. \n",
       "  * Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. [Bleu: A Method for Automatic Evaluation of Machine Translation](https://www.aclweb.org/anthology/P02-1040). In _ACL 2002_ , pages 311–318. \n",
       "  * Pereyra et al. (2017) Gabriel Pereyra, George Tucker, Jan Chorowski, Lukasz Kaiser, and Geoffrey E. Hinton. 2017. [Regularizing neural networks by penalizing confident output distributions](https://openreview.net/forum?id=HyhbYrGYe). In _5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Workshop Track Proceedings_. \n",
       "  * Radford et al. (2018a) Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018a. [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf). \n",
       "  * Radford et al. (2018b) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2018b. [Language models are unsupervised multitask learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf). \n",
       "  * Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](http://arxiv.org/abs/1910.10683). _J. Mach. Learn. Res._ , 21:140:1–140:67. \n",
       "  * Rajpurkar et al. (2018) Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. [Know What You Don’t Know: Unanswerable Questions for SQuAD](https://aclanthology.org/P18-2124). In _ACL 2018_ , pages 784–789. \n",
       "  * Rajpurkar et al. (2016) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. [Squad: 100, 000+ questions for machine comprehension of text](https://doi.org/10.18653/v1/d16-1264). In _EMNLP 2016_ , pages 2383–2392. \n",
       "  * Rasley et al. (2020) Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. [Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters](https://doi.org/10.1145/3394486.3406703). In _KDD 2020_ , pages 3505–3506. \n",
       "  * Rush et al. (2015) Alexander M. Rush, Sumit Chopra, and Jason Weston. 2015. [A neural attention model for abstractive sentence summarization](https://aclanthology.org/D15-1044). In _EMNLP 2015_ , pages 379–389. \n",
       "  * Schick and Schütze (2020a) Timo Schick and Hinrich Schütze. 2020a. [Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference](https://aclanthology.org/2021.eacl-main.20/). pages 255–269. \n",
       "  * Schick and Schütze (2020b) Timo Schick and Hinrich Schütze. 2020b. [It’s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners](https://doi.org/10.18653/v1/2021.naacl-main.185). pages 2339–2352. \n",
       "  * See et al. (2017) Abigail See, Peter J. Liu, and Christopher D. Manning. 2017. [Get To The Point: Summarization with Pointer-Generator Networks](https://doi.org/10.18653/v1/P17-1099). In _ACL 2017_ , pages 1073–1083. \n",
       "  * Shen et al. (2020) Tianxiao Shen, Victor Quach, Regina Barzilay, and Tommi S. Jaakkola. 2020. [Blank language models](https://doi.org/10.18653/v1/2020.emnlp-main.420). pages 5186–5198. \n",
       "  * Shoeybi et al. (2019) Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019. [Megatron-lm: Training multi-billion parameter language models using model parallelism](http://arxiv.org/abs/1909.08053). _CoRR_ , abs/1909.08053. \n",
       "  * Socher et al. (2013) Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](https://aclanthology.org/D13-1170). In _EMNLP 2013_ , pages 1631–1642. \n",
       "  * Song et al. (2019) Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. 2019. [MASS: Masked Sequence to Sequence Pre-training for Language Generation](http://proceedings.mlr.press/v97/song19d.html). In _ICML 2019_ , volume 97, pages 5926–5936. \n",
       "  * Trinh and Le (2019) Trieu H. Trinh and Quoc V. Le. 2019. [A Simple Method for Commonsense Reasoning](http://arxiv.org/abs/1806.02847). _arXiv:1806.02847 [cs]_. \n",
       "  * Wang et al. (2019) Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. [SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems](https://proceedings.neurips.cc/paper/2019/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html). In _NeurIPS 2019_ , pages 3261–3275. \n",
       "  * Wang et al. (2018) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. [GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding](https://openreview.net/forum?id=rJ4km2R5t7). In _ICLR 2019_ , pages 353–355. \n",
       "  * Williams et al. (2018) Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. [A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference](https://doi.org/10.18653/v1/n18-1101). In _NAACL 2018_ , pages 1112–1122. \n",
       "  * Yang et al. (2019) Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. 2019. [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://papers.nips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf). In _NeurIPS 2019_ , pages 5754–5764. \n",
       "  * Yang et al. (2017) Zichao Yang, Zhiting Hu, Ruslan Salakhutdinov, and Taylor Berg-Kirkpatrick. 2017\\. [Improved variational autoencoders for text modeling using dilated convolutions](http://proceedings.mlr.press/v70/yang17d.html). In _ICML 2017_ , volume 70, pages 3881–3890. \n",
       "  * Zhang et al. (2020) Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. 2020. [PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](http://proceedings.mlr.press/v119/zhang20ae.html). In _ICML 2020_ , pages 11328–11339. \n",
       "  * Zhu et al. (2019) Wanrong Zhu, Zhiting Hu, and Eric Xing. 2019. [Text infilling](http://arxiv.org/abs/1901.00158). _arXiv preprint arXiv:1901.00158_. \n",
       "  * Zhu et al. (2015) Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. [Aligning books and movies: Towards story-like visual explanations by watching movies and reading books](https://doi.org/10.1109/ICCV.2015.11). In _ICCV 2015_ , pages 19–27. \n",
       "\n",
       "##  Appendix A Pretraining Setting\n",
       "\n",
       "###  A.1 Datasets\n",
       "\n",
       "To train GLM and GLM, we use BookCorpus Zhu et al. (2015) and Wikipedia used\n",
       "by BERT Devlin et al. (2019).\n",
       "\n",
       "To train GLM, we follow the pretraining datasets of RoBERTa Liu et al. (2019),\n",
       "which consist of BookCorups Zhu et al. (2015),Wikipedia (16GB), CC-News (the\n",
       "English portion of the CommonCrawl News\n",
       "dataset333<https://commoncrawl.org/2016/10/news-dataset-available> 76GB),\n",
       "OpenWebText (web content extracted from URLs shared on Reddit with at least\n",
       "three upvotesGokaslan and Cohen (2019), 38GB) and Stories (subset of\n",
       "CommonCrawl data filtered to match the story-like style of Winograd schemas\n",
       "Trinh and Le (2019), 31GB). The Stories dataset is no longer publicly\n",
       "available444<https://github.com/tensorflow/models/tree/archive/research/lm_commonsense#1-download-\n",
       "data-files>. Therefore, we remove the Stories dataset and replace OpenWebText\n",
       "with OpenWebText2555<https://openwebtext2.readthedocs.io/en/latest> (66GB).\n",
       "The CC-News dataset is not publicly available and we use the CC-News-en\n",
       "published by Mackenzie et al. (2020). All the datasets used total 158GB of\n",
       "uncompressed texts, close in size to RoBERTa’s 160GB datasets.\n",
       "\n",
       "###  A.2 Hyperparameters\n",
       "\n",
       "Table 7: Hyperparameters for pretraining Hyperparameters | GLM  | GLM  | GLM   \n",
       "---|---|---|---  \n",
       "Number of Layers | 12 | 24 | 24  \n",
       "Hidden size | 768 | 1024 | 1024  \n",
       "FFN inner hidden size | 3072 | 4096 | 4096  \n",
       "Attention heads | 12 | 16 | 16  \n",
       "Attention head size | 64 | 64 | 64  \n",
       "Dropout | 0.1 | 0.1 | 0.1  \n",
       "Attention Dropout | 0.1 | 0.1 | 0.1  \n",
       "Warmup Steps | 6k | 8k | 30K  \n",
       "Peak Learning Rate | 4e-4 | 2e-4 | 4e-4  \n",
       "Batch Size | 1024 | 1024 | 8192  \n",
       "Weight Decay | 0.1 | 0.1 | 0.01  \n",
       "Max Steps | 120k | 200k | 250k  \n",
       "Learning Rate Decay | Cosine | Cosine | Cosine  \n",
       "Adam  | 1e-6 | 1e-6 | 1e-6  \n",
       "Adam  | 0.9 | 0.9 | 0.9  \n",
       "Adam  | 0.98 | 0.98 | 0.98  \n",
       "Gradient Clipping | 1.0 | 1.0 | 1.0  \n",
       "  \n",
       "The hyperparameters for GLM and GLM are similar to those used by BERT. For\n",
       "trade-off of training speed and fair comparison with BERT (batch size 256 and\n",
       "1,000,000 training steps), we use batch size of 1024 and 200,000 training\n",
       "steps for GLM. Since GLM is smaller, we reduce the number of training steps to\n",
       "120,000 to speed up pre-training. The hyperparameters for GLM and GLM are the\n",
       "same as those of GLM. The hyperparameters except Transformer architecture for\n",
       "GLM and GLM are the same as those of GLM. The models are trained on 64 V100\n",
       "GPUs for 200K steps with batch size of 1024 and maximum sequence length of\n",
       "512, which takes about 2.5 days for GLM.\n",
       "\n",
       "To train GLM, we follow most of the hyperparameters of RoBERTa. The main\n",
       "difference includes: (1) Due to resource limit, we only pre-train GLM  for\n",
       "250,000 steps, which are half of RoBERTa and BART’s training steps, and close\n",
       "to T5 in number of trained tokens. (2) We use cosine decay instead of linear\n",
       "decay for learning rate scheduling (3) We additionally apply gradient clipping\n",
       "with value 1.0.\n",
       "\n",
       "The hyperparameters for all the pre-training settings are summarized in Table\n",
       "7.\n",
       "\n",
       "###  A.3 Implementation\n",
       "\n",
       "Our pretraining implementation is based on Megatron-LM Shoeybi et al. (2019)\n",
       "and DeepSpeed Rasley et al. (2020). We include our code in the supplementary\n",
       "material. Due to the size limit of supplementary material, we cannot include\n",
       "the pretrained models, but will make them public available in the future.\n",
       "\n",
       "##  Appendix B Downstream Tasks\n",
       "\n",
       "###  B.1 SuperGLUE\n",
       "\n",
       "The SuperGLUE benchmark consists of 8 NLU tasks. We formulate them as blank\n",
       "infilling tasks, following Schick and Schütze (2020b). Table 8 shows the cloze\n",
       "questions and verbalizers we used in our experiments. For 3 tasks (ReCoRD,\n",
       "COPA, and WSC), the answer may consist of multiple tokens, and for the other 5\n",
       "tasks, the answer is always a single token.\n",
       "\n",
       "When finetuning GLM on the SuperGLUE tasks, we construct the input using the\n",
       "cloze questions in Table 8 and replace the blank with a [MASK] token. Then we\n",
       "compute the score of generating each answer candidate. For the 5 single-token\n",
       "tasks, the score is defined to be the logit of the verbalizer token. For the 3\n",
       "multi-token tasks, we use the sum of the log-probabilities of the verbalizer\n",
       "tokens. Thanks to the autoregressive blank infilling mechanism we proposed, we\n",
       "can obtain all the log-probabilities in one pass. Then we compute the cross\n",
       "entropy loss using the groundtruth label and update the model parameters.\n",
       "\n",
       "For the baseline classifiers, we follow the standard practice to concatenate\n",
       "the input parts of each task (such as the premise and hypothesis for textual\n",
       "entailment, or the passage, question and answer for ReCORD and MultiRC) and\n",
       "add a classification layer on top of the [CLS] token representation. We also\n",
       "implemented cloze-style finetuning for the other pre-trained models, but the\n",
       "performance was usually similar to the standard classifier, as we shown in the\n",
       "ablation study. Models with blank-infilling objectives, such as T5 and our\n",
       "GLM, benefits more from converting the NLU tasks into cloze questions. Thus\n",
       "for T5 and GLM, we report the performance after such conversion in our main\n",
       "results.\n",
       "\n",
       "Table 8: Cloze questions and verbalizers for the 8 SuperGLUE tasks used in our experiments. ∗ denotes the answer contains multiple tokens. Dataset | Task |  Cloze Question  |  Verbalizers   \n",
       "---|---|---|---  \n",
       "ReCoRD∗ | Question answering |  [passage ] [cloze question ]  |  Answer candidates   \n",
       "COPA∗ | Causal reasoning |  “[choice ]” or “[choice ]”? [premise ], so .  |  /   \n",
       "WSC∗ | Coreference resolution |  [sentence ] The pronoun ‘’ refers to .  |  Noun   \n",
       "RTE | Textual entailment |  “[hypothesis ]”?  , “[premise ]”  |  “yes” (entailment), “no” (not entailment)   \n",
       "BoolQ | Question answering |  [passage ]. Question: ? Answer: .  |  “yes” / “no”   \n",
       "WiC | Word sense disambiguation |  “[sentence ]” / “[sentence ]” Similar sense of [word ]? .  |  “yes” / “no”   \n",
       "CB | Textual entailment |  “[hypothesis ]”?  , “[premise ]”  |  “yes” (entailment), “no” (contradiction), “maybe” (neutral)   \n",
       "MultiRC | Question answering |  [passage ]. Question: ? Is it [answer ]? .  |  “yes” / “no”   \n",
       "  \n",
       "###  B.2 Sequence-to-Sequence\n",
       "\n",
       "Fot the text summarization task, we use the dataset Gigaword Rush et al.\n",
       "(2015) for model fine-tuning and evaluation. We finetune GLM on the training\n",
       "set for 4 epochs with AdamW optimizer. The learning rate has a peak value of\n",
       "3e-5, warm-up over the 6% training steps and a linear decay. We also use label\n",
       "smoothing with rate 0.1 Pereyra et al. (2017). The maximum document length is\n",
       "192 and the maximum summary length is 32. During decoding, we use beam search\n",
       "with beam size of 5 and remove repeated trigrams. We tweak the value of length\n",
       "penalty on the development set. The evaluation metrics are the F1 scores of\n",
       "Rouge-1, Rouge-2, and Rouge-L Lin (2004) on the test set.\n",
       "\n",
       "For the question generation task, we use the SQuAD 1.1 dataset Rajpurkar et\n",
       "al. (2016) and follow the dataset split of Du et al. (2017). The optimizer\n",
       "hyperparameters are the same as those of abstractive summarization. The\n",
       "maximum passage length is 464 and the maximum question length is 48. During\n",
       "decoding, we use beam search with beam size 5 and tweak the value of length\n",
       "penalty on the development set. The evaluation metrics are the scores of\n",
       "BLEU-1, BLEU-2, BLEU-3, BLEU-4 Papineni et al. (2002), METEOR Denkowski and\n",
       "Lavie (2014) and Rouge-L Lin (2004).\n",
       "\n",
       "Results of T5 on XSum are obtained by running the summarization script\n",
       "provided by Huggingface\n",
       "transformers666<https://github.com/huggingface/transformers/tree/master/examples/pytorch/summarization>.\n",
       "All the other results of baselines on seq2seq tasks are obtained from the\n",
       "corresponding papers.\n",
       "\n",
       "###  B.3 Text Infilling\n",
       "\n",
       "We follow Shen et al. (2020) and evaluate text infilling performance on the\n",
       "Yahoo Answers dataset Yang et al. (2017), which contains 100K/10K/10K\n",
       "documents for train/valid/test respectively. The average document length is 78\n",
       "words. To construct the text infilling task, we randomly mask a given ratio\n",
       "of each document’s tokens and the contiguous masked tokens are collapsed into\n",
       "a single blank. We finetune GLM on the training set for 5 epochs with dynamic\n",
       "masking, i.e. the blanks are randomly generated at training time. Similar to\n",
       "the sequence-to-sequence experiments, we use an AdamW optimizer with a peak\n",
       "learning rate 1e-5 and 6% warm-up linear scheduler.\n",
       "\n",
       "For comparison with previous work, we use the same test set constructed by\n",
       "Shen et al. (2020). The evaluation metric is the BLEU score of the infilled\n",
       "text against the original document. We compare with two baselines: (1) BERT,\n",
       "which learns a left-to-right language model to generate the masked tokens on\n",
       "top of the blank representation, and (2) BLM proposed by Shen et al. (2020),\n",
       "which can fill in the blank with arbitrary trajectories.\n",
       "\n",
       "###  B.4 Language Modeling\n",
       "\n",
       "We evaluate the model’s ability of language modeling with perplexity on\n",
       "BookWiki and accuracy on the LAMBDA dataset Paperno et al. (2016).\n",
       "\n",
       "Perplexity is an evaluation criterion that has been well studied for language\n",
       "modeling. Perplexity is the exponentiation of the average cross entropy of a\n",
       "corpus.\n",
       "\n",
       "|  |  | (4)  \n",
       "---|---|---|---  \n",
       "  \n",
       "where . Since transformers can only operate on a window of fixed input size ,\n",
       "we cannot fully calculate  and can only calculate . Even calculating this\n",
       "value for each token is prohibitively expensive, since we need to conduct\n",
       "evaluations of -size contexts. To improve evaluation efficiency, we adopt\n",
       "_overlapping evaluation_ , where we advance the sliding windows by some\n",
       "overlap  each time and only compute the cross entropy loss for the last\n",
       "tokens of the window. In our experiments we set  for all the models.\n",
       "\n",
       "LAMBDA is a cloze-style dataset to test the ability of long-range dependency\n",
       "modeling. Each example is a passage consisting of 4-5 sentences with the last\n",
       "word missing and the model is required to predict the last word of the\n",
       "passage. Since we use WordPiece tokenization, a word can be split into several\n",
       "subword units. We use teacher forcing and consider the prediction correct only\n",
       "when all the predicted tokens are correct.\n",
       "\n",
       "##  Appendix C Results on Other NLU Benchmarks\n",
       "\n",
       "GLUE Wang et al. (2018) is another widely-used NLU benchmark, including single\n",
       "sentence tasks (e.g. sentiment analysis Socher et al. (2013)) and sentence\n",
       "pair tasks (e.g. text similarity Cer et al. (2017) and natural language\n",
       "inference Williams et al. (2018); Dagan et al. (2005)). The benchmark is\n",
       "usually considered as less challenging than SuperGLUE. SQuAD Rajpurkar et al.\n",
       "(2016, 2018) is an extractive question answering benchmark. We further compare\n",
       "GLM with BERT on the two benchmarks.\n",
       "\n",
       "The results on GLUE and SQuAD are shown in Tables 9 and 10. On the two\n",
       "benchmarks, GLM can still outperform BERT with the same amount of parameters,\n",
       "but with a smaller margin.\n",
       "\n",
       "Table 9: Results on the GLUE dev set. Model | MNLI | QNLI | QQP | RTE | SST-2 | MRPC | CoLA | STS-B | Avg  \n",
       "---|---|---|---|---|---|---|---|---|---  \n",
       "BERT | 86.6 | 92.3 | 91.3 | 73.6 | 93.2 | 88.0 | 60.6 | 90.0 | 84.4  \n",
       "GLM | 86.7 | 92.8 | 91.5 | 74.0 | 93.5 | 90.0 | 61.4 | 90.7 | 85.1  \n",
       "Table 10: Results on the SQuAD v1.1/v2.0 dev sets. Model |  SQuAD v1.1 EM/F1  |  SQuAD v2.0 EM/F1   \n",
       "---|---|---  \n",
       "BERT | 80.8 / 88.5 | 73.7 / 76.3  \n",
       "GLM |  81.5 / 88.6  |  74.7 / 77.8   \n",
       "BERT | 84.1 / 90.9 | 79.0 / 81.8  \n",
       "GLM |  85.4 / 91.6  |  80.3 / 83.3   \n",
       "  \n",
       "##  Appendix D Text Generation Samples\n",
       "\n",
       "We show texts generated by GLM given unseen contexts randomly sampled from the\n",
       "test set. We use top- random sampling with  for generation and set maximum\n",
       "sequence length to 512. Some of the texts are cut short.\n",
       "\n",
       "######  Example D.1.\n",
       "\n",
       "The Wyoming State Legislature is the legislative branch of the U.S. State of\n",
       "Wyoming.\n",
       "\n",
       "GLM: It consists of a state senate and a house of representatives. As of 2019,\n",
       "state senators are Steve Kish, Joe L. Williams, and Linda S. Bollens. Members\n",
       "of the Wyoming State Legislature are elected from single-member districts\n",
       "representing the majority of the state. The current state senate members are:\n",
       "In recent years, there have been four changes to the senate. The most recent\n",
       "is the creation of a six-seat district that includes all or part of the\n",
       "following: In the 2009 elections, the state senate members were elected to\n",
       "six-year terms. The current state house members are: The Wyoming Constitution\n",
       "assigns certain powers to the governor. Most notably, the governor is\n",
       "president of the senate and governor. However, if the governor desires to\n",
       "appoint a member to the Wyoming state senate, a law authorizes the governor to\n",
       "do so. The governor of Wyoming holds no legislative power but has the power to\n",
       "veto lawmakers, which is not limited to the veto of laws. Under the wyoming\n",
       "state constitution, the governor can veto the actions of the other members of\n",
       "the wyoming house of representatives. The governor can also appoint members of\n",
       "the wyoming senate. In addition, the governor can appoint members of the\n",
       "Wyoming house of representatives. Wyoming’s constitution provides that the\n",
       "governor can appoint a member of the wyoming state senate to the wyoming\n",
       "supreme court, and the chairman of the wyoming senate.\n",
       "\n",
       "######  Example D.2.\n",
       "\n",
       "Jonathan Terry is a television and film actor.\n",
       "\n",
       "GLM: He was a voice actor for the “X-Men\" cartoon series. He was also a voice\n",
       "actor for “the Simpsons\" as well as “the marvelous misadventures of superman.\n",
       "He also has voiced characters in “the legend of korra\" He has appeared on\n",
       "several television series, including “the simpsons\", “the x-files\", “heroes\"\n",
       "and “the simpsons movie\" as the character captain billy higgledy-pig, and the\n",
       "web series “krusty mysteries\" as the character Colonel Trungus. He wrote for\n",
       "“the Guardian\" newspaper. Jonathan Terry was born in London. Terry grew up in\n",
       "Surrey, England and attended the University of Sussex in the United Kingdom,\n",
       "graduating with a degree in english literature. He was a guest lecturer at\n",
       "King’s College London, and then took two years of acting courses at the brit\n",
       "school of acting to prepare for his future career in the entertainment\n",
       "industry. Terry first appeared in the TV series “the Simpsons\" as the\n",
       "character captain Billy Higgledy-pig, but his character was only a one-time\n",
       "recurring character in the series’ first six seasons. He later appeared as a\n",
       "regular for the show’s final six seasons, and has been a frequent guest in the\n",
       "show since. He appeared in the first few episodes of “\" as the character major\n",
       "Jack Ryan. He has also appeared as part of the supporting cast of several\n",
       "episodes of “the secret life of pets\". He has also worked on “the simpsons\" TV\n",
       "show since “the simpsons movie\", most notably playing the roles of Captain\n",
       "Skeletor and the ghost of the same name. He plays characters in several films,\n",
       "including “\", “\", “\" and “\". He has appeared in music videos for the killers\n",
       "in 1993, the pretenders in 1995, and in the TV shows “the royal\" and “the\n",
       "bill\".\n",
       "\n",
       "######  Example D.3.\n",
       "\n",
       "Corona was a station along the port Washington branch of the long island rail\n",
       "road in the Corona section of queens, New York City. It was one of two\n",
       "stations built by the flushing railroad in Corona, this one having been at\n",
       "Grand Avenue (later called National Avenue, now National Street ) and 45th\n",
       "Avenue.\n",
       "\n",
       "GLM: It had a brick entrance building next to the tracks, and one platform\n",
       "with 2 side platforms. The station burned down and was rebuilt in 1908. The\n",
       "station closed on september 15, 1927, with the train service transferred from\n",
       "Grand Avenue to 45th Avenue. There were plans to build a subway extension to\n",
       "Corona, but it was never built. In the 1980s, two stations were constructed on\n",
       "the line, Corona Road and Corona Park. The new Corona station opened in 1988,\n",
       "and the original Corona station was demolished.\n",
       "\n",
       "######  Example D.4.\n",
       "\n",
       "Robert Lee Smith (born July 5, 1938) is a former American football cornerback\n",
       "in the national football league. He played for the Los Angeles Rams\n",
       "(1962–1965) and the Detroit Lions (1965–1966). He was also a kick and punt\n",
       "returner.\n",
       "\n",
       "GLM: In his four-year NFL career, he played in 33 games and started 14,\n",
       "registering 62 career interceptions. He completed his NFL career with five\n",
       "interceptions. Smith was born in La Canada Flintridge, Michigan, in 1938. He\n",
       "attended Bishop O’ Dowd high school in Flintridge. Smith’ s father was a\n",
       "pharmacist. He graduated from Michigan State University in 1958 with a degree\n",
       "in business administration. Smith played for the Michigan Wolverines football\n",
       "team from 1959 to 1963. In his two years as a reserve cornerback, he led the\n",
       "conference in interceptions with five. In 1962, he set the Wolverines’ all-\n",
       "time interception record with 13, and was second overall in the 1962 season’s\n",
       "Heisman Trophy voting. Smith also won the Dick Butkus award as the nation’s\n",
       "outstanding linebacker. In 1961, the “Los Angeles Times\" wrote that Smith “is\n",
       "an outstanding pass rusher, with an average of almost 100 yards per punt\n",
       "return.\" Smith was inducted into the university of Michigan athletic hall of\n",
       "honor in 1989 and the national football foundation hall of fame in 1991. He\n",
       "was elected to the Michigan sports hall of fame in 1995. Smith earned the\n",
       "honor because of his accomplishments prior to his NFL career. He was one of\n",
       "four Michigan players honored as first-overall selections in the 1964 NFL\n",
       "draft. The others were Joe Namath, Bill Nelsen, and Jerry Kramer. In 1966, the\n",
       "NFL gave players $300,000 a season to play football. After his rookie season,\n",
       "he was not selected to play in the 1966 pro bowl. On January 13, 1966, the\n",
       "Rams traded smith to the Detroit Lions for Paul Hornung, and later that year\n",
       "he was traded to the Lions for Ray “the Lion\" Jones in exchange for Linebacker\n",
       "Jim “the Hawk\" Johnson. On September 10, 1968, he was traded back to Los\n",
       "Angeles for a second round pick in the 1970 draft. He was also traded to the\n",
       "St. Louis Cardinals for a second round pick in the 1970 draft. On June 2, 1970\n",
       "he was cut by the Cardinals. On November 15, 1970, the Los Angeles Rams\n",
       "acquired Smith from the Lions in exchange for Linebacker Tony Harris. The Rams\n",
       "waived Smith during the September 1, 1972 offseason. Smith’s number at\n",
       "Michigan State was # 7 in 1969.\n",
       "\n",
       "[◄](/html/2103.10359) [![](/assets/ar5iv.png)](/) [Feeling  \n",
       "lucky?](/feeling_lucky) [Conversion  \n",
       "report](/log/2103.10360) [Report  \n",
       "an issue](https://github.com/dginev/ar5iv/issues/new?template=improve-article\n",
       "--arxiv-id-.md&title=Improve+article+2103.10360) [View original  \n",
       "on arXiv](https://arxiv.org/abs/2103.10360)[►](/html/2103.10361)\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import html2text\n",
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "def html_to_markdown(html_content):\n",
    "    # Convert HTML to Markdown using html2text\n",
    "    markdown_content = html2text.html2text(html_content)\n",
    "    return markdown_content\n",
    "\n",
    "\n",
    "# Example usage\n",
    "markdown_content = html_to_markdown(cleaned_html)\n",
    "Markdown(markdown_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
